Init dist using slurm!
Job Id is 17140001 on c05r4n[07-15],c07r4n[13-19],c08r2n[00-19],c09r3n[14-19],c09r4n[00-19],c10r2n[00-01] 
rank: 0 world size: 256 addr: c05r4n07  port: 9912
[2024-07-12 15:55:55,699] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.708949 18055 ProcessGroupNCCL.cpp:835] [Rank 3] NCCL watchdog thread started!
I0712 15:55:55.708926 17134 ProcessGroupNCCL.cpp:669] [Rank 3] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.709326 18058 ProcessGroupNCCL.cpp:835] [Rank 1] NCCL watchdog thread started!
I0712 15:55:55.709316 17132 ProcessGroupNCCL.cpp:669] [Rank 1] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.709466 18059 ProcessGroupNCCL.cpp:835] [Rank 2] NCCL watchdog thread started!
I0712 15:55:55.709465 17133 ProcessGroupNCCL.cpp:669] [Rank 2] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.730722  2300 ProcessGroupNCCL.cpp:835] [Rank 31] NCCL watchdog thread started!
I0712 15:55:55.730710  1522 ProcessGroupNCCL.cpp:669] [Rank 31] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.730870  2301 ProcessGroupNCCL.cpp:835] [Rank 28] NCCL watchdog thread started!
I0712 15:55:55.730865  1519 ProcessGroupNCCL.cpp:669] [Rank 28] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.731184  1520 ProcessGroupNCCL.cpp:669] [Rank 29] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:55.731209  2303 ProcessGroupNCCL.cpp:835] [Rank 29] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.731748  1521 ProcessGroupNCCL.cpp:669] [Rank 30] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:55.731753  2305 ProcessGroupNCCL.cpp:835] [Rank 30] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.740264  7556 ProcessGroupNCCL.cpp:835] [Rank 17] NCCL watchdog thread started!
I0712 15:55:55.740257  6668 ProcessGroupNCCL.cpp:669] [Rank 17] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.740617  7558 ProcessGroupNCCL.cpp:835] [Rank 18] NCCL watchdog thread started!
I0712 15:55:55.740613  6669 ProcessGroupNCCL.cpp:669] [Rank 18] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.740948  7561 ProcessGroupNCCL.cpp:835] [Rank 16] NCCL watchdog thread started!
I0712 15:55:55.740922  6667 ProcessGroupNCCL.cpp:669] [Rank 16] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.741055  7562 ProcessGroupNCCL.cpp:835] [Rank 19] NCCL watchdog thread started!
I0712 15:55:55.741030  6670 ProcessGroupNCCL.cpp:669] [Rank 19] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.754336 11545 ProcessGroupNCCL.cpp:835] [Rank 9] NCCL watchdog thread started!
I0712 15:55:55.754333 10642 ProcessGroupNCCL.cpp:669] [Rank 9] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.754375 11546 ProcessGroupNCCL.cpp:835] [Rank 8] NCCL watchdog thread started!
I0712 15:55:55.754374 10641 ProcessGroupNCCL.cpp:669] [Rank 8] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.754535 10643 ProcessGroupNCCL.cpp:669] [Rank 10] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.754529 10644 ProcessGroupNCCL.cpp:669] [Rank 11] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:55.754535 11548 ProcessGroupNCCL.cpp:835] [Rank 10] NCCL watchdog thread started!
I0712 15:55:55.754552 11547 ProcessGroupNCCL.cpp:835] [Rank 11] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.830487  1896 ProcessGroupNCCL.cpp:669] [Rank 4] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:55.830497  2782 ProcessGroupNCCL.cpp:835] [Rank 4] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.830888  2785 ProcessGroupNCCL.cpp:835] [Rank 6] NCCL watchdog thread started!
I0712 15:55:55.830879  1898 ProcessGroupNCCL.cpp:669] [Rank 6] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.830981  2786 ProcessGroupNCCL.cpp:835] [Rank 7] NCCL watchdog thread started!
I0712 15:55:55.830956  1899 ProcessGroupNCCL.cpp:669] [Rank 7] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.831586  2788 ProcessGroupNCCL.cpp:835] [Rank 5] NCCL watchdog thread started!
I0712 15:55:55.831584  1897 ProcessGroupNCCL.cpp:669] [Rank 5] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.851931 20118 ProcessGroupNCCL.cpp:835] [Rank 13] NCCL watchdog thread started!
I0712 15:55:55.851926 19186 ProcessGroupNCCL.cpp:669] [Rank 13] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.852452 19188 ProcessGroupNCCL.cpp:669] [Rank 15] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:55.852458 20121 ProcessGroupNCCL.cpp:835] [Rank 15] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.852612 20122 ProcessGroupNCCL.cpp:835] [Rank 12] NCCL watchdog thread started!
I0712 15:55:55.852607 19185 ProcessGroupNCCL.cpp:669] [Rank 12] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.853477 20124 ProcessGroupNCCL.cpp:835] [Rank 14] NCCL watchdog thread started!
I0712 15:55:55.853471 19187 ProcessGroupNCCL.cpp:669] [Rank 14] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.959690 31469 ProcessGroupNCCL.cpp:835] [Rank 24] NCCL watchdog thread started!
I0712 15:55:55.959687 30538 ProcessGroupNCCL.cpp:669] [Rank 24] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.960319 31472 ProcessGroupNCCL.cpp:835] [Rank 25] NCCL watchdog thread started!
I0712 15:55:55.960317 30539 ProcessGroupNCCL.cpp:669] [Rank 25] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.960536 31473 ProcessGroupNCCL.cpp:835] [Rank 27] NCCL watchdog thread started!
I0712 15:55:55.960531 30541 ProcessGroupNCCL.cpp:669] [Rank 27] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:55.961172 30540 ProcessGroupNCCL.cpp:669] [Rank 26] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:55.961199 31475 ProcessGroupNCCL.cpp:835] [Rank 26] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.165614 13750 ProcessGroupNCCL.cpp:669] [Rank 159] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.165673 14698 ProcessGroupNCCL.cpp:835] [Rank 159] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.165659 14697 ProcessGroupNCCL.cpp:835] [Rank 156] NCCL watchdog thread started!
I0712 15:55:56.165637 13747 ProcessGroupNCCL.cpp:669] [Rank 156] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.165702 14699 ProcessGroupNCCL.cpp:835] [Rank 158] NCCL watchdog thread started!
I0712 15:55:56.165696 13749 ProcessGroupNCCL.cpp:669] [Rank 158] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.165732 14700 ProcessGroupNCCL.cpp:835] [Rank 157] NCCL watchdog thread started!
I0712 15:55:56.165733 13748 ProcessGroupNCCL.cpp:669] [Rank 157] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.278898 23455 ProcessGroupNCCL.cpp:835] [Rank 199] NCCL watchdog thread started!
I0712 15:55:56.278863 22576 ProcessGroupNCCL.cpp:669] [Rank 199] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.279263 23456 ProcessGroupNCCL.cpp:835] [Rank 197] NCCL watchdog thread started!
I0712 15:55:56.279251 22574 ProcessGroupNCCL.cpp:669] [Rank 197] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.279338 23457 ProcessGroupNCCL.cpp:835] [Rank 196] NCCL watchdog thread started!
I0712 15:55:56.279330 22573 ProcessGroupNCCL.cpp:669] [Rank 196] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.279453 23458 ProcessGroupNCCL.cpp:835] [Rank 198] NCCL watchdog thread started!
I0712 15:55:56.279426 22575 ProcessGroupNCCL.cpp:669] [Rank 198] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.291935 11357 ProcessGroupNCCL.cpp:835] [Rank 81] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.292285 16393 ProcessGroupNCCL.cpp:835] [Rank 66] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.291980 11358 ProcessGroupNCCL.cpp:835] [Rank 82] NCCL watchdog thread started!
I0712 15:55:56.291919 10348 ProcessGroupNCCL.cpp:669] [Rank 81] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.292192 11359 ProcessGroupNCCL.cpp:835] [Rank 80] NCCL watchdog thread started!
I0712 15:55:56.291986 10349 ProcessGroupNCCL.cpp:669] [Rank 82] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.292269 11360 ProcessGroupNCCL.cpp:835] [Rank 83] NCCL watchdog thread started!
I0712 15:55:56.292160 10347 ProcessGroupNCCL.cpp:669] [Rank 80] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.292230 10350 ProcessGroupNCCL.cpp:669] [Rank 83] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.293751 16395 ProcessGroupNCCL.cpp:835] [Rank 65] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.293802 16394 ProcessGroupNCCL.cpp:835] [Rank 64] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.293923 16392 ProcessGroupNCCL.cpp:835] [Rank 67] NCCL watchdog thread started!
I0712 15:55:56.294595 15413 ProcessGroupNCCL.cpp:669] [Rank 66] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.294612 15412 ProcessGroupNCCL.cpp:669] [Rank 65] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.294631 15411 ProcessGroupNCCL.cpp:669] [Rank 64] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.294636 15414 ProcessGroupNCCL.cpp:669] [Rank 67] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.295562 13745 ProcessGroupNCCL.cpp:835] [Rank 137] NCCL watchdog thread started!
I0712 15:55:56.295544 12848 ProcessGroupNCCL.cpp:669] [Rank 137] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.295949 12847 ProcessGroupNCCL.cpp:669] [Rank 136] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.296072 13747 ProcessGroupNCCL.cpp:835] [Rank 138] NCCL watchdog thread started!
I0712 15:55:56.296030 13746 ProcessGroupNCCL.cpp:835] [Rank 136] NCCL watchdog thread started!
I0712 15:55:56.296048 12849 ProcessGroupNCCL.cpp:669] [Rank 138] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.296133 13748 ProcessGroupNCCL.cpp:835] [Rank 139] NCCL watchdog thread started!
I0712 15:55:56.296124 12850 ProcessGroupNCCL.cpp:669] [Rank 139] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.296375 23465 ProcessGroupNCCL.cpp:835] [Rank 122] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.296448 23466 ProcessGroupNCCL.cpp:835] [Rank 123] NCCL watchdog thread started!
I0712 15:55:56.296384 22662 ProcessGroupNCCL.cpp:669] [Rank 122] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.296427 22663 ProcessGroupNCCL.cpp:669] [Rank 123] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.296933 23467 ProcessGroupNCCL.cpp:835] [Rank 120] NCCL watchdog thread started!
I0712 15:55:56.296911 22660 ProcessGroupNCCL.cpp:669] [Rank 120] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.297006 23468 ProcessGroupNCCL.cpp:835] [Rank 121] NCCL watchdog thread started!
I0712 15:55:56.297003 22661 ProcessGroupNCCL.cpp:669] [Rank 121] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.300905 27005 ProcessGroupNCCL.cpp:835] [Rank 174] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.301223 29682 ProcessGroupNCCL.cpp:835] [Rank 85] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.302140 29683 ProcessGroupNCCL.cpp:835] [Rank 87] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.302387 27006 ProcessGroupNCCL.cpp:835] [Rank 173] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.302558 27007 ProcessGroupNCCL.cpp:835] [Rank 172] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.302645 27004 ProcessGroupNCCL.cpp:835] [Rank 175] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.302817 29685 ProcessGroupNCCL.cpp:835] [Rank 86] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.302860 29684 ProcessGroupNCCL.cpp:835] [Rank 84] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.305032  1573 ProcessGroupNCCL.cpp:835] [Rank 102] NCCL watchdog thread started!
I0712 15:55:56.305023   701 ProcessGroupNCCL.cpp:669] [Rank 102] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.305128  1574 ProcessGroupNCCL.cpp:835] [Rank 101] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.305220  1575 ProcessGroupNCCL.cpp:835] [Rank 100] NCCL watchdog thread started!
I0712 15:55:56.305124   700 ProcessGroupNCCL.cpp:669] [Rank 101] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.305202   699 ProcessGroupNCCL.cpp:669] [Rank 100] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.305451  1576 ProcessGroupNCCL.cpp:835] [Rank 103] NCCL watchdog thread started!
I0712 15:55:56.305445   702 ProcessGroupNCCL.cpp:669] [Rank 103] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.307924 18187 ProcessGroupNCCL.cpp:835] [Rank 148] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.307991 18188 ProcessGroupNCCL.cpp:835] [Rank 151] NCCL watchdog thread started!
I0712 15:55:56.307909 17282 ProcessGroupNCCL.cpp:669] [Rank 148] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.307987 17285 ProcessGroupNCCL.cpp:669] [Rank 151] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.308357 18190 ProcessGroupNCCL.cpp:835] [Rank 149] NCCL watchdog thread started!
I0712 15:55:56.308351 17283 ProcessGroupNCCL.cpp:669] [Rank 149] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.308394 18189 ProcessGroupNCCL.cpp:835] [Rank 150] NCCL watchdog thread started!
I0712 15:55:56.308384 17284 ProcessGroupNCCL.cpp:669] [Rank 150] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.308704  9174 ProcessGroupNCCL.cpp:835] [Rank 195] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309038 29213 ProcessGroupNCCL.cpp:835] [Rank 77] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309265 29214 ProcessGroupNCCL.cpp:835] [Rank 78] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309310 29215 ProcessGroupNCCL.cpp:835] [Rank 79] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309374 29212 ProcessGroupNCCL.cpp:835] [Rank 76] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309489 11215 ProcessGroupNCCL.cpp:835] [Rank 203] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309695 11216 ProcessGroupNCCL.cpp:835] [Rank 200] NCCL watchdog thread started!
I0712 15:55:56.309453 10223 ProcessGroupNCCL.cpp:669] [Rank 203] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309664  9175 ProcessGroupNCCL.cpp:835] [Rank 194] NCCL watchdog thread started!
I0712 15:55:56.309641 10220 ProcessGroupNCCL.cpp:669] [Rank 200] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309747  9173 ProcessGroupNCCL.cpp:835] [Rank 193] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309720 11217 ProcessGroupNCCL.cpp:835] [Rank 202] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.309762 11218 ProcessGroupNCCL.cpp:835] [Rank 201] NCCL watchdog thread started!
I0712 15:55:56.309715 10222 ProcessGroupNCCL.cpp:669] [Rank 202] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.309741 10221 ProcessGroupNCCL.cpp:669] [Rank 201] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310318 26110 ProcessGroupNCCL.cpp:669] [Rank 174] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310248 28778 ProcessGroupNCCL.cpp:669] [Rank 85] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.310484 27040 ProcessGroupNCCL.cpp:835] [Rank 167] NCCL watchdog thread started!
I0712 15:55:56.310477 26253 ProcessGroupNCCL.cpp:669] [Rank 167] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310335 26109 ProcessGroupNCCL.cpp:669] [Rank 173] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310269 28780 ProcessGroupNCCL.cpp:669] [Rank 87] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310359 26108 ProcessGroupNCCL.cpp:669] [Rank 172] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310294 28779 ProcessGroupNCCL.cpp:669] [Rank 86] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310366 26111 ProcessGroupNCCL.cpp:669] [Rank 175] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310307 28777 ProcessGroupNCCL.cpp:669] [Rank 84] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.310750 27041 ProcessGroupNCCL.cpp:835] [Rank 165] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.310827 27042 ProcessGroupNCCL.cpp:835] [Rank 164] NCCL watchdog thread started!
I0712 15:55:56.310829 26250 ProcessGroupNCCL.cpp:669] [Rank 164] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310770 26251 ProcessGroupNCCL.cpp:669] [Rank 165] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.311101 27043 ProcessGroupNCCL.cpp:835] [Rank 166] NCCL watchdog thread started!
I0712 15:55:56.311118 26252 ProcessGroupNCCL.cpp:669] [Rank 166] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.311226  9176 ProcessGroupNCCL.cpp:835] [Rank 192] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.310734 30840 ProcessGroupNCCL.cpp:835] [Rank 227] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.310950 30841 ProcessGroupNCCL.cpp:835] [Rank 226] NCCL watchdog thread started!
I0712 15:55:56.310715 29992 ProcessGroupNCCL.cpp:669] [Rank 227] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.311364 28354 ProcessGroupNCCL.cpp:669] [Rank 77] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.311097 30842 ProcessGroupNCCL.cpp:835] [Rank 224] NCCL watchdog thread started!
I0712 15:55:56.311380 28355 ProcessGroupNCCL.cpp:669] [Rank 78] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.311081 29989 ProcessGroupNCCL.cpp:669] [Rank 224] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.311389 28356 ProcessGroupNCCL.cpp:669] [Rank 79] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.310943 29991 ProcessGroupNCCL.cpp:669] [Rank 226] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.311403 28353 ProcessGroupNCCL.cpp:669] [Rank 76] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.311146 30843 ProcessGroupNCCL.cpp:835] [Rank 225] NCCL watchdog thread started!
I0712 15:55:56.311125 29990 ProcessGroupNCCL.cpp:669] [Rank 225] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.312072  8268 ProcessGroupNCCL.cpp:669] [Rank 195] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.312093  8267 ProcessGroupNCCL.cpp:669] [Rank 194] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.312109  8265 ProcessGroupNCCL.cpp:669] [Rank 192] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.312106  8266 ProcessGroupNCCL.cpp:669] [Rank 193] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.314565 12697 ProcessGroupNCCL.cpp:835] [Rank 115] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.314643 20786 ProcessGroupNCCL.cpp:835] [Rank 106] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.315938 12696 ProcessGroupNCCL.cpp:835] [Rank 112] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.316731 12699 ProcessGroupNCCL.cpp:835] [Rank 113] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.317135 12698 ProcessGroupNCCL.cpp:835] [Rank 114] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.316537 20785 ProcessGroupNCCL.cpp:835] [Rank 104] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.316596 20787 ProcessGroupNCCL.cpp:835] [Rank 105] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.317145 20788 ProcessGroupNCCL.cpp:835] [Rank 107] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321035 11395 ProcessGroupNCCL.cpp:835] [Rank 239] NCCL watchdog thread started!
I0712 15:55:56.321934 11103 ProcessGroupNCCL.cpp:669] [Rank 115] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321954 11100 ProcessGroupNCCL.cpp:669] [Rank 112] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321966 11101 ProcessGroupNCCL.cpp:669] [Rank 113] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321982 11102 ProcessGroupNCCL.cpp:669] [Rank 114] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321018  4164 ProcessGroupNCCL.cpp:835] [Rank 254] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321568 11394 ProcessGroupNCCL.cpp:835] [Rank 237] NCCL watchdog thread started!
I0712 15:55:56.321010  3310 ProcessGroupNCCL.cpp:669] [Rank 254] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321578 11396 ProcessGroupNCCL.cpp:835] [Rank 238] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321713 16833 ProcessGroupNCCL.cpp:835] [Rank 48] NCCL watchdog thread started!
I0712 15:55:56.321702 19918 ProcessGroupNCCL.cpp:669] [Rank 104] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321542  4165 ProcessGroupNCCL.cpp:835] [Rank 253] NCCL watchdog thread started!
I0712 15:55:56.321692 19920 ProcessGroupNCCL.cpp:669] [Rank 106] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321842 17633 ProcessGroupNCCL.cpp:835] [Rank 161] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321880   659 ProcessGroupNCCL.cpp:835] [Rank 118] NCCL watchdog thread started!
I0712 15:55:56.321533  3309 ProcessGroupNCCL.cpp:669] [Rank 253] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321720 19921 ProcessGroupNCCL.cpp:669] [Rank 107] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321835 16659 ProcessGroupNCCL.cpp:669] [Rank 161] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321645  4166 ProcessGroupNCCL.cpp:835] [Rank 252] NCCL watchdog thread started!
I0712 15:55:56.321722 19919 ProcessGroupNCCL.cpp:669] [Rank 105] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322069 17634 ProcessGroupNCCL.cpp:835] [Rank 160] NCCL watchdog thread started!
I0712 15:55:56.321642  3308 ProcessGroupNCCL.cpp:669] [Rank 252] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322062 16658 ProcessGroupNCCL.cpp:669] [Rank 160] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321863 32205 ProcessGroupNCCL.cpp:669] [Rank 118] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.321662  4167 ProcessGroupNCCL.cpp:835] [Rank 255] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322212 11397 ProcessGroupNCCL.cpp:835] [Rank 236] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322232  8438 ProcessGroupNCCL.cpp:835] [Rank 155] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322327 17635 ProcessGroupNCCL.cpp:835] [Rank 162] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322139 32206 ProcessGroupNCCL.cpp:669] [Rank 119] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.321652  3311 ProcessGroupNCCL.cpp:669] [Rank 255] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322175 14654 ProcessGroupNCCL.cpp:835] [Rank 62] NCCL watchdog thread started!
I0712 15:55:56.322290 16660 ProcessGroupNCCL.cpp:669] [Rank 162] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322324   662 ProcessGroupNCCL.cpp:835] [Rank 116] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322319 14655 ProcessGroupNCCL.cpp:835] [Rank 63] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322413 17636 ProcessGroupNCCL.cpp:835] [Rank 163] NCCL watchdog thread started!
I0712 15:55:56.322245   660 ProcessGroupNCCL.cpp:835] [Rank 119] NCCL watchdog thread started!
I0712 15:55:56.322168 13723 ProcessGroupNCCL.cpp:669] [Rank 62] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322410 16661 ProcessGroupNCCL.cpp:669] [Rank 163] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322268 32203 ProcessGroupNCCL.cpp:669] [Rank 116] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322383   661 ProcessGroupNCCL.cpp:835] [Rank 117] NCCL watchdog thread started!
I0712 15:55:56.322634 10387 ProcessGroupNCCL.cpp:669] [Rank 239] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322604 16834 ProcessGroupNCCL.cpp:835] [Rank 51] NCCL watchdog thread started!
I0712 15:55:56.322300 13724 ProcessGroupNCCL.cpp:669] [Rank 63] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322357 32204 ProcessGroupNCCL.cpp:669] [Rank 117] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322662 10385 ProcessGroupNCCL.cpp:669] [Rank 237] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322418 14656 ProcessGroupNCCL.cpp:835] [Rank 60] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322477 11196 ProcessGroupNCCL.cpp:835] [Rank 183] NCCL watchdog thread started!
I0712 15:55:56.322474 10158 ProcessGroupNCCL.cpp:669] [Rank 183] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322682 10384 ProcessGroupNCCL.cpp:669] [Rank 236] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322409 13721 ProcessGroupNCCL.cpp:669] [Rank 60] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322556 11198 ProcessGroupNCCL.cpp:835] [Rank 181] NCCL watchdog thread started!
I0712 15:55:56.322685 10386 ProcessGroupNCCL.cpp:669] [Rank 238] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322868 16832 ProcessGroupNCCL.cpp:835] [Rank 49] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322933  8437 ProcessGroupNCCL.cpp:835] [Rank 154] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322523 14657 ProcessGroupNCCL.cpp:835] [Rank 61] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322537 11197 ProcessGroupNCCL.cpp:835] [Rank 182] NCCL watchdog thread started!
I0712 15:55:56.322513 10157 ProcessGroupNCCL.cpp:669] [Rank 182] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322517 13722 ProcessGroupNCCL.cpp:669] [Rank 61] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.322580 11199 ProcessGroupNCCL.cpp:835] [Rank 180] NCCL watchdog thread started!
I0712 15:55:56.322531 10156 ProcessGroupNCCL.cpp:669] [Rank 181] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.322574 10155 ProcessGroupNCCL.cpp:669] [Rank 180] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.323263 16835 ProcessGroupNCCL.cpp:835] [Rank 50] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.323356  8440 ProcessGroupNCCL.cpp:835] [Rank 152] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.323369  8439 ProcessGroupNCCL.cpp:835] [Rank 153] NCCL watchdog thread started!
I0712 15:55:56.326743 15840 ProcessGroupNCCL.cpp:669] [Rank 48] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326872  7561 ProcessGroupNCCL.cpp:669] [Rank 155] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326752 15843 ProcessGroupNCCL.cpp:669] [Rank 51] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326913  7560 ProcessGroupNCCL.cpp:669] [Rank 154] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326771 15841 ProcessGroupNCCL.cpp:669] [Rank 49] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326918  7558 ProcessGroupNCCL.cpp:669] [Rank 152] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326776 15842 ProcessGroupNCCL.cpp:669] [Rank 50] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.326927  7559 ProcessGroupNCCL.cpp:669] [Rank 153] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.330056 27865 ProcessGroupNCCL.cpp:835] [Rank 36] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.331189 27866 ProcessGroupNCCL.cpp:835] [Rank 38] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.331243 27868 ProcessGroupNCCL.cpp:835] [Rank 37] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.331694 27867 ProcessGroupNCCL.cpp:835] [Rank 39] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332037 10156 ProcessGroupNCCL.cpp:835] [Rank 91] NCCL watchdog thread started!
I0712 15:55:56.332032  9022 ProcessGroupNCCL.cpp:669] [Rank 91] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332417 17023 ProcessGroupNCCL.cpp:835] [Rank 128] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332095 10157 ProcessGroupNCCL.cpp:835] [Rank 88] NCCL watchdog thread started!
I0712 15:55:56.332090  9019 ProcessGroupNCCL.cpp:669] [Rank 88] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332162 10158 ProcessGroupNCCL.cpp:835] [Rank 89] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332206 10159 ProcessGroupNCCL.cpp:835] [Rank 90] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332790 17020 ProcessGroupNCCL.cpp:835] [Rank 131] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332341  6941 ProcessGroupNCCL.cpp:835] [Rank 41] NCCL watchdog thread started!
I0712 15:55:56.332157  9020 ProcessGroupNCCL.cpp:669] [Rank 89] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332520  3505 ProcessGroupNCCL.cpp:835] [Rank 215] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332883 17021 ProcessGroupNCCL.cpp:835] [Rank 130] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332494  6942 ProcessGroupNCCL.cpp:835] [Rank 40] NCCL watchdog thread started!
I0712 15:55:56.332194  9021 ProcessGroupNCCL.cpp:669] [Rank 90] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.333029 17022 ProcessGroupNCCL.cpp:835] [Rank 129] NCCL watchdog thread started!
I0712 15:55:56.332285  6006 ProcessGroupNCCL.cpp:669] [Rank 41] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.332477  6005 ProcessGroupNCCL.cpp:669] [Rank 40] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332734  6943 ProcessGroupNCCL.cpp:835] [Rank 42] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332370 28828 ProcessGroupNCCL.cpp:835] [Rank 97] NCCL watchdog thread started!
I0712 15:55:56.332710  6007 ProcessGroupNCCL.cpp:669] [Rank 42] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.332841  6944 ProcessGroupNCCL.cpp:835] [Rank 43] NCCL watchdog thread started!
I0712 15:55:56.332834  6008 ProcessGroupNCCL.cpp:669] [Rank 43] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.333281 28827 ProcessGroupNCCL.cpp:835] [Rank 99] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.333891 31399 ProcessGroupNCCL.cpp:835] [Rank 108] NCCL watchdog thread started!
I0712 15:55:56.333885 30225 ProcessGroupNCCL.cpp:669] [Rank 108] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.333842  3508 ProcessGroupNCCL.cpp:835] [Rank 213] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.333356 28826 ProcessGroupNCCL.cpp:835] [Rank 98] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.334008  3507 ProcessGroupNCCL.cpp:835] [Rank 214] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.334424 31401 ProcessGroupNCCL.cpp:835] [Rank 110] NCCL watchdog thread started!
I0712 15:55:56.334416 30227 ProcessGroupNCCL.cpp:669] [Rank 110] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.334483 31402 ProcessGroupNCCL.cpp:835] [Rank 111] NCCL watchdog thread started!
I0712 15:55:56.334479 30228 ProcessGroupNCCL.cpp:669] [Rank 111] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.334578  3506 ProcessGroupNCCL.cpp:835] [Rank 212] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.334657 31403 ProcessGroupNCCL.cpp:835] [Rank 109] NCCL watchdog thread started!
I0712 15:55:56.334654 30226 ProcessGroupNCCL.cpp:669] [Rank 109] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.334534 28825 ProcessGroupNCCL.cpp:835] [Rank 96] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335218  1358 ProcessGroupNCCL.cpp:835] [Rank 170] NCCL watchdog thread started!
I0712 15:55:56.335217   477 ProcessGroupNCCL.cpp:669] [Rank 170] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335603  1359 ProcessGroupNCCL.cpp:835] [Rank 171] NCCL watchdog thread started!
I0712 15:55:56.335585   478 ProcessGroupNCCL.cpp:669] [Rank 171] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335866  1360 ProcessGroupNCCL.cpp:835] [Rank 169] NCCL watchdog thread started!
I0712 15:55:56.335829   476 ProcessGroupNCCL.cpp:669] [Rank 169] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335898  1361 ProcessGroupNCCL.cpp:835] [Rank 168] NCCL watchdog thread started!
I0712 15:55:56.335892   475 ProcessGroupNCCL.cpp:669] [Rank 168] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335543  7169 ProcessGroupNCCL.cpp:835] [Rank 57] NCCL watchdog thread started!
I0712 15:55:56.335549  6245 ProcessGroupNCCL.cpp:669] [Rank 57] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335808  7170 ProcessGroupNCCL.cpp:835] [Rank 59] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.335888  7171 ProcessGroupNCCL.cpp:835] [Rank 58] NCCL watchdog thread started!
I0712 15:55:56.335757  6247 ProcessGroupNCCL.cpp:669] [Rank 59] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.336257  9964 ProcessGroupNCCL.cpp:835] [Rank 188] NCCL watchdog thread started!
I0712 15:55:56.335860  6246 ProcessGroupNCCL.cpp:669] [Rank 58] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.336129  7172 ProcessGroupNCCL.cpp:835] [Rank 56] NCCL watchdog thread started!
I0712 15:55:56.336223  8867 ProcessGroupNCCL.cpp:669] [Rank 188] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336093  6244 ProcessGroupNCCL.cpp:669] [Rank 56] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.336602  9965 ProcessGroupNCCL.cpp:835] [Rank 190] NCCL watchdog thread started!
I0712 15:55:56.336941 16068 ProcessGroupNCCL.cpp:669] [Rank 128] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336947 16071 ProcessGroupNCCL.cpp:669] [Rank 131] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.336666  9966 ProcessGroupNCCL.cpp:835] [Rank 191] NCCL watchdog thread started!
I0712 15:55:56.336971 16070 ProcessGroupNCCL.cpp:669] [Rank 130] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336570  8869 ProcessGroupNCCL.cpp:669] [Rank 190] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336947 27035 ProcessGroupNCCL.cpp:669] [Rank 36] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336984 16069 ProcessGroupNCCL.cpp:669] [Rank 129] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336643  8870 ProcessGroupNCCL.cpp:669] [Rank 191] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336969 27037 ProcessGroupNCCL.cpp:669] [Rank 38] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.336712  9967 ProcessGroupNCCL.cpp:835] [Rank 189] NCCL watchdog thread started!
I0712 15:55:56.336982 27036 ProcessGroupNCCL.cpp:669] [Rank 37] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336709  8868 ProcessGroupNCCL.cpp:669] [Rank 189] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.336988 27038 ProcessGroupNCCL.cpp:669] [Rank 39] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.337604 24126 ProcessGroupNCCL.cpp:835] [Rank 243] NCCL watchdog thread started!
I0712 15:55:56.337600 23249 ProcessGroupNCCL.cpp:669] [Rank 243] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.337620 23248 ProcessGroupNCCL.cpp:669] [Rank 242] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.337622 24127 ProcessGroupNCCL.cpp:835] [Rank 242] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.337646 23247 ProcessGroupNCCL.cpp:669] [Rank 241] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.337898 26516 ProcessGroupNCCL.cpp:835] [Rank 221] NCCL watchdog thread started!
I0712 15:55:56.337659 24128 ProcessGroupNCCL.cpp:835] [Rank 241] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.337993 24129 ProcessGroupNCCL.cpp:835] [Rank 240] NCCL watchdog thread started!
I0712 15:55:56.337961 23246 ProcessGroupNCCL.cpp:669] [Rank 240] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338362  2603 ProcessGroupNCCL.cpp:669] [Rank 215] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.337919 27926 ProcessGroupNCCL.cpp:669] [Rank 97] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338378  2601 ProcessGroupNCCL.cpp:669] [Rank 213] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.337944 27928 ProcessGroupNCCL.cpp:669] [Rank 99] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338402  2602 ProcessGroupNCCL.cpp:669] [Rank 214] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.337960 27927 ProcessGroupNCCL.cpp:669] [Rank 98] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338409  2600 ProcessGroupNCCL.cpp:669] [Rank 212] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.337970 27925 ProcessGroupNCCL.cpp:669] [Rank 96] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338585  6357 ProcessGroupNCCL.cpp:835] [Rank 127] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338430 26517 ProcessGroupNCCL.cpp:835] [Rank 223] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338706  6359 ProcessGroupNCCL.cpp:835] [Rank 124] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338646 26518 ProcessGroupNCCL.cpp:835] [Rank 220] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338855  6358 ProcessGroupNCCL.cpp:835] [Rank 126] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338812 10303 ProcessGroupNCCL.cpp:835] [Rank 187] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338886  6360 ProcessGroupNCCL.cpp:835] [Rank 125] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338891 10304 ProcessGroupNCCL.cpp:835] [Rank 186] NCCL watchdog thread started!
I0712 15:55:56.338804  9357 ProcessGroupNCCL.cpp:669] [Rank 187] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338873  9356 ProcessGroupNCCL.cpp:669] [Rank 186] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338927   376 ProcessGroupNCCL.cpp:835] [Rank 177] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338858 26515 ProcessGroupNCCL.cpp:835] [Rank 222] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339201 10305 ProcessGroupNCCL.cpp:835] [Rank 184] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.338974   377 ProcessGroupNCCL.cpp:835] [Rank 178] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339073 32184 ProcessGroupNCCL.cpp:835] [Rank 231] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339237 18264 ProcessGroupNCCL.cpp:669] [Rank 143] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339215  9354 ProcessGroupNCCL.cpp:669] [Rank 184] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338917 31991 ProcessGroupNCCL.cpp:669] [Rank 177] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339036 31265 ProcessGroupNCCL.cpp:669] [Rank 231] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339387 10306 ProcessGroupNCCL.cpp:835] [Rank 185] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339411 32185 ProcessGroupNCCL.cpp:835] [Rank 230] NCCL watchdog thread started!
I0712 15:55:56.339382  9355 ProcessGroupNCCL.cpp:669] [Rank 185] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.338969 31992 ProcessGroupNCCL.cpp:669] [Rank 178] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339035 11919 ProcessGroupNCCL.cpp:835] [Rank 47] NCCL watchdog thread started!
I0712 15:55:56.339320 19222 ProcessGroupNCCL.cpp:835] [Rank 143] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339263 31993 ProcessGroupNCCL.cpp:669] [Rank 179] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339154 11920 ProcessGroupNCCL.cpp:835] [Rank 46] NCCL watchdog thread started!
I0712 15:55:56.339394 31264 ProcessGroupNCCL.cpp:669] [Rank 230] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339025 11048 ProcessGroupNCCL.cpp:669] [Rank 47] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339398 25617 ProcessGroupNCCL.cpp:669] [Rank 220] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339651 32186 ProcessGroupNCCL.cpp:835] [Rank 229] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339778 19223 ProcessGroupNCCL.cpp:835] [Rank 140] NCCL watchdog thread started!
I0712 15:55:56.339316   378 ProcessGroupNCCL.cpp:835] [Rank 179] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339592 30957 ProcessGroupNCCL.cpp:835] [Rank 68] NCCL watchdog thread started!
I0712 15:55:56.339147 11047 ProcessGroupNCCL.cpp:669] [Rank 46] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339416 25618 ProcessGroupNCCL.cpp:669] [Rank 221] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339622 31263 ProcessGroupNCCL.cpp:669] [Rank 229] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339725 18261 ProcessGroupNCCL.cpp:669] [Rank 140] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339435 25620 ProcessGroupNCCL.cpp:669] [Rank 223] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339690 32187 ProcessGroupNCCL.cpp:835] [Rank 228] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339286 32749 ProcessGroupNCCL.cpp:835] [Rank 145] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339869 19224 ProcessGroupNCCL.cpp:835] [Rank 141] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339759   379 ProcessGroupNCCL.cpp:835] [Rank 176] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339560 11921 ProcessGroupNCCL.cpp:835] [Rank 44] NCCL watchdog thread started!
I0712 15:55:56.339442 25619 ProcessGroupNCCL.cpp:669] [Rank 222] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339682 31262 ProcessGroupNCCL.cpp:669] [Rank 228] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339864 18262 ProcessGroupNCCL.cpp:669] [Rank 141] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339704 31990 ProcessGroupNCCL.cpp:669] [Rank 176] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339555 11045 ProcessGroupNCCL.cpp:669] [Rank 44] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339916 19225 ProcessGroupNCCL.cpp:835] [Rank 142] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339612 11922 ProcessGroupNCCL.cpp:835] [Rank 45] NCCL watchdog thread started!
I0712 15:55:56.339911 18263 ProcessGroupNCCL.cpp:669] [Rank 142] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339607 11046 ProcessGroupNCCL.cpp:669] [Rank 45] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339840  3603 ProcessGroupNCCL.cpp:835] [Rank 55] NCCL watchdog thread started!
I0712 15:55:56.340283  5404 ProcessGroupNCCL.cpp:669] [Rank 125] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.340297  5405 ProcessGroupNCCL.cpp:669] [Rank 126] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.340312  5406 ProcessGroupNCCL.cpp:669] [Rank 127] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.339854  2758 ProcessGroupNCCL.cpp:669] [Rank 55] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339987 32746 ProcessGroupNCCL.cpp:835] [Rank 146] NCCL watchdog thread started!
I0712 15:55:56.340320  5403 ProcessGroupNCCL.cpp:669] [Rank 124] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340210  3604 ProcessGroupNCCL.cpp:835] [Rank 53] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.339843  3433 ProcessGroupNCCL.cpp:835] [Rank 132] NCCL watchdog thread started!
I0712 15:55:56.340188  2756 ProcessGroupNCCL.cpp:669] [Rank 53] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340410  3605 ProcessGroupNCCL.cpp:835] [Rank 54] NCCL watchdog thread started!
I0712 15:55:56.339805  2524 ProcessGroupNCCL.cpp:669] [Rank 132] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.340381  2757 ProcessGroupNCCL.cpp:669] [Rank 54] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340623 30955 ProcessGroupNCCL.cpp:835] [Rank 69] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340330 32748 ProcessGroupNCCL.cpp:835] [Rank 147] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340389  3697 ProcessGroupNCCL.cpp:835] [Rank 75] NCCL watchdog thread started!
I0712 15:55:56.340353  2711 ProcessGroupNCCL.cpp:669] [Rank 75] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340515  3606 ProcessGroupNCCL.cpp:835] [Rank 52] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340312  3434 ProcessGroupNCCL.cpp:835] [Rank 133] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340390 32747 ProcessGroupNCCL.cpp:835] [Rank 144] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340427  3698 ProcessGroupNCCL.cpp:835] [Rank 72] NCCL watchdog thread started!
I0712 15:55:56.340422  2708 ProcessGroupNCCL.cpp:669] [Rank 72] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.340507  2755 ProcessGroupNCCL.cpp:669] [Rank 52] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340339  3435 ProcessGroupNCCL.cpp:835] [Rank 134] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340478  3699 ProcessGroupNCCL.cpp:835] [Rank 73] NCCL watchdog thread started!
I0712 15:55:56.340251  2525 ProcessGroupNCCL.cpp:669] [Rank 133] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340494  3700 ProcessGroupNCCL.cpp:835] [Rank 74] NCCL watchdog thread started!
I0712 15:55:56.340297  2526 ProcessGroupNCCL.cpp:669] [Rank 134] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.340456  2709 ProcessGroupNCCL.cpp:669] [Rank 73] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340366  3436 ProcessGroupNCCL.cpp:835] [Rank 135] NCCL watchdog thread started!
I0712 15:55:56.340487  2710 ProcessGroupNCCL.cpp:669] [Rank 74] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.340166 16748 ProcessGroupNCCL.cpp:835] [Rank 219] NCCL watchdog thread started!
I0712 15:55:56.340359  2527 ProcessGroupNCCL.cpp:669] [Rank 135] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.341519 30954 ProcessGroupNCCL.cpp:835] [Rank 71] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.341697 30956 ProcessGroupNCCL.cpp:835] [Rank 70] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.341701 16750 ProcessGroupNCCL.cpp:835] [Rank 217] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.341784 16751 ProcessGroupNCCL.cpp:835] [Rank 218] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.341823 16749 ProcessGroupNCCL.cpp:835] [Rank 216] NCCL watchdog thread started!
I0712 15:55:56.342430 31778 ProcessGroupNCCL.cpp:669] [Rank 145] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.342453 31780 ProcessGroupNCCL.cpp:669] [Rank 147] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.342587  2002 ProcessGroupNCCL.cpp:835] [Rank 211] NCCL watchdog thread started!
I0712 15:55:56.342449 31779 ProcessGroupNCCL.cpp:669] [Rank 146] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.342465 31777 ProcessGroupNCCL.cpp:669] [Rank 144] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.343051 15814 ProcessGroupNCCL.cpp:669] [Rank 219] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.343073 15812 ProcessGroupNCCL.cpp:669] [Rank 217] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.343101 15811 ProcessGroupNCCL.cpp:669] [Rank 216] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.343094 15813 ProcessGroupNCCL.cpp:669] [Rank 218] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.343351  2004 ProcessGroupNCCL.cpp:835] [Rank 210] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.343361  2003 ProcessGroupNCCL.cpp:835] [Rank 209] NCCL watchdog thread started!
I0712 15:55:56.344230 30012 ProcessGroupNCCL.cpp:669] [Rank 68] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.344246 30013 ProcessGroupNCCL.cpp:669] [Rank 69] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.344264 30015 ProcessGroupNCCL.cpp:669] [Rank 71] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.344276 30014 ProcessGroupNCCL.cpp:669] [Rank 70] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.344396  2001 ProcessGroupNCCL.cpp:835] [Rank 208] NCCL watchdog thread started!
I0712 15:55:56.344849  1148 ProcessGroupNCCL.cpp:669] [Rank 211] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.344885  1145 ProcessGroupNCCL.cpp:669] [Rank 208] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.344873  1146 ProcessGroupNCCL.cpp:669] [Rank 209] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.344882  1147 ProcessGroupNCCL.cpp:669] [Rank 210] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.350302  2095 ProcessGroupNCCL.cpp:835] [Rank 248] NCCL watchdog thread started!
I0712 15:55:56.350298  1166 ProcessGroupNCCL.cpp:669] [Rank 248] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.350322  2096 ProcessGroupNCCL.cpp:835] [Rank 250] NCCL watchdog thread started!
I0712 15:55:56.350319  1168 ProcessGroupNCCL.cpp:669] [Rank 250] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.350441  2097 ProcessGroupNCCL.cpp:835] [Rank 249] NCCL watchdog thread started!
I0712 15:55:56.350461  1167 ProcessGroupNCCL.cpp:669] [Rank 249] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.350486  2098 ProcessGroupNCCL.cpp:835] [Rank 251] NCCL watchdog thread started!
I0712 15:55:56.350476  1169 ProcessGroupNCCL.cpp:669] [Rank 251] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.351459  2447 ProcessGroupNCCL.cpp:835] [Rank 246] NCCL watchdog thread started!
I0712 15:55:56.351419  1506 ProcessGroupNCCL.cpp:669] [Rank 246] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.351852  2448 ProcessGroupNCCL.cpp:835] [Rank 244] NCCL watchdog thread started!
I0712 15:55:56.351840  1504 ProcessGroupNCCL.cpp:669] [Rank 244] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.351977  2449 ProcessGroupNCCL.cpp:835] [Rank 247] NCCL watchdog thread started!
I0712 15:55:56.351970  1507 ProcessGroupNCCL.cpp:669] [Rank 247] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.352064  2450 ProcessGroupNCCL.cpp:835] [Rank 245] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.351922  4815 ProcessGroupNCCL.cpp:835] [Rank 235] NCCL watchdog thread started!
I0712 15:55:56.352061  1505 ProcessGroupNCCL.cpp:669] [Rank 245] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.351918  3968 ProcessGroupNCCL.cpp:669] [Rank 235] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.352416  4816 ProcessGroupNCCL.cpp:835] [Rank 234] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.352629  4817 ProcessGroupNCCL.cpp:835] [Rank 232] NCCL watchdog thread started!
I0712 15:55:56.352574  3965 ProcessGroupNCCL.cpp:669] [Rank 232] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.352393  3967 ProcessGroupNCCL.cpp:669] [Rank 234] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.352656  4818 ProcessGroupNCCL.cpp:835] [Rank 233] NCCL watchdog thread started!
I0712 15:55:56.352648  3966 ProcessGroupNCCL.cpp:669] [Rank 233] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.361676 18290 ProcessGroupNCCL.cpp:835] [Rank 94] NCCL watchdog thread started!
I0712 15:55:56.361676 17404 ProcessGroupNCCL.cpp:669] [Rank 94] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.362026 18293 ProcessGroupNCCL.cpp:835] [Rank 92] NCCL watchdog thread started!
I0712 15:55:56.362022 17402 ProcessGroupNCCL.cpp:669] [Rank 92] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.361922 18291 ProcessGroupNCCL.cpp:835] [Rank 93] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.361994 18292 ProcessGroupNCCL.cpp:835] [Rank 95] NCCL watchdog thread started!
I0712 15:55:56.361912 17403 ProcessGroupNCCL.cpp:669] [Rank 93] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.361999 17405 ProcessGroupNCCL.cpp:669] [Rank 95] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.366026  9169 ProcessGroupNCCL.cpp:835] [Rank 206] NCCL watchdog thread started!
I0712 15:55:56.366025  8286 ProcessGroupNCCL.cpp:669] [Rank 206] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.366406  9170 ProcessGroupNCCL.cpp:835] [Rank 207] NCCL watchdog thread started!
I0712 15:55:56.366398  8287 ProcessGroupNCCL.cpp:669] [Rank 207] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.366649  9171 ProcessGroupNCCL.cpp:835] [Rank 204] NCCL watchdog thread started!
I0712 15:55:56.366644  8284 ProcessGroupNCCL.cpp:669] [Rank 204] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.366709  9172 ProcessGroupNCCL.cpp:835] [Rank 205] NCCL watchdog thread started!
I0712 15:55:56.366703  8285 ProcessGroupNCCL.cpp:669] [Rank 205] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.656765 28648 ProcessGroupNCCL.cpp:835] [Rank 34] NCCL watchdog thread started!
I0712 15:55:56.656740 27716 ProcessGroupNCCL.cpp:669] [Rank 34] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.656973 28649 ProcessGroupNCCL.cpp:835] [Rank 35] NCCL watchdog thread started!
I0712 15:55:56.656949 27717 ProcessGroupNCCL.cpp:669] [Rank 35] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.657377 28651 ProcessGroupNCCL.cpp:835] [Rank 33] NCCL watchdog thread started!
I0712 15:55:56.657369 27715 ProcessGroupNCCL.cpp:669] [Rank 33] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.657850 27714 ProcessGroupNCCL.cpp:669] [Rank 32] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 15:55:56.657876 28653 ProcessGroupNCCL.cpp:835] [Rank 32] NCCL watchdog thread started!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.699461 12906 ProcessGroupNCCL.cpp:835] [Rank 23] NCCL watchdog thread started!
I0712 15:55:56.699450 11830 ProcessGroupNCCL.cpp:669] [Rank 23] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.699877 12908 ProcessGroupNCCL.cpp:835] [Rank 22] NCCL watchdog thread started!
I0712 15:55:56.699884 11829 ProcessGroupNCCL.cpp:669] [Rank 22] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.700289 12910 ProcessGroupNCCL.cpp:835] [Rank 21] NCCL watchdog thread started!
I0712 15:55:56.700284 11828 ProcessGroupNCCL.cpp:669] [Rank 21] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.700781 12912 ProcessGroupNCCL.cpp:835] [Rank 20] NCCL watchdog thread started!
I0712 15:55:56.700776 11827 ProcessGroupNCCL.cpp:669] [Rank 20] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0712 15:55:56.702776 18061 ProcessGroupNCCL.cpp:835] [Rank 0] NCCL watchdog thread started!
I0712 15:55:56.702767 17131 ProcessGroupNCCL.cpp:669] [Rank 0] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
rank: 146 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 6 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 21 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 156 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 78 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 219 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 200 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 216 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 5 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 201 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 145 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 23 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 103 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 144 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 217 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 224 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 225 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r3n17,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 3 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 147 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 22 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 14 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 77 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 226 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 101 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 1 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 25 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 34 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 7 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 20 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 15 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 79 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 141 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 54 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n12,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n08,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 102 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 158 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 2 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 24 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 35 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r3n14,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 4 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 143 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 53 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n14,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 19 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 221 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 214 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 157 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 26 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n12,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 142 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 243 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 213 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 159 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 131 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 74 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 29 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 189 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 218 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 242 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 203 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 45 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 215 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 128 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 75 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 76 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 140 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 55 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 190 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 227 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 240 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 202 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 46 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 100 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 223 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 130 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 73 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 30 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 52 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 191 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 64 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 241 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 16 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 129 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 32 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 72 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 13 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 188 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 67 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 209 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 212 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 9 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 27 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 33 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 31 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 199 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 65 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 17 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 44 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 222 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 208 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 10 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n07,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 28 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n08,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 197 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 66 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 18 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 47 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 210 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 8 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 104 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 12 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 196 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 220 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 211 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 11 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 107 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n16,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n15,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n03,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n19,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c07r4n17,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n05,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 198 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n00,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n18,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n11,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n09,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n11,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 106 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n10,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 105 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n14,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n07,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c07r4n15,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n09,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n10,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c05r4n10,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 115 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 113 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 114 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 112 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 62 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n12,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 63 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 61 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 60 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c07r4n19,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,798 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,798 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,798 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,798 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,798 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,799 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,799 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,799 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,799 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,797 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,799 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,799 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,799 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,799 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,800 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,800 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,798 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,800 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,800 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 244 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,800 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 174 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,801 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,801 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,802 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,801 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n19,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,801 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,802 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,802 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,802 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,802 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,800 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,802 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,802 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,803 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,803 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,803 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,803 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,804 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,803 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,804 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,805 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 173 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,805 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 175 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,806 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 172 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,806 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,806 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,806 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n01,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,806 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 245 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 247 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,807 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,807 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,807 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,807 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 246 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,807 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,807 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,807 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,808 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,808 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,808 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,809 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,809 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,808 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,809 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,809 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,808 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,809 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,809 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,809 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,809 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,810 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,809 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,810 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,810 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,810 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,810 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,810 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,810 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,810 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 48 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,810 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 49 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 51 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 183 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 192 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 161 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,811 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 193 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,811 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,811 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,811 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 181 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 195 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,811 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,811 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c07r4n16,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,811 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,811 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 133 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,811 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 180 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n06,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 50 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 182 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,812 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 194 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,812 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 42 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n03,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 134 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 132 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 135 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 162 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 163 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 160 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 43 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 41 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n17,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 40 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 232 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 233 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 235 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 249 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r3n18,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 185 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 251 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c07r4n14,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 248 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 186 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 234 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 152 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 239 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n16,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 187 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 184 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 237 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 238 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c10r2n00,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 236 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 250 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r3n16,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n04,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 97 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 98 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n17,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 99 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 153 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 155 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 154 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 96 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,818 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n08,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,819 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,819 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,819 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,820 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 38 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,820 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,820 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 36 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,820 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 39 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,821 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 37 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 86 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 151 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c07r4n13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 84 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,821 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,821 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 85 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,821 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 87 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 149 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 150 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 148 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 169 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 170 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n05,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 171 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r3n15,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 168 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 93 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 95 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 111 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 94 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 92 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n00,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 57 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 110 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 58 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,823 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 109 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 59 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 108 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 56 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n07,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c07r4n18,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n11,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,824 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,824 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,825 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 69 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 230 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 71 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 229 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 70 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 231 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,825 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,825 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 68 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,825 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 228 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,826 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,826 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 253 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,825 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,826 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,825 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,826 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,826 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n01,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 252 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r4n15,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 255 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,826 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,826 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,826 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 254 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,826 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,827 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,826 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,826 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,827 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 125 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c10r2n01,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,827 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 127 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,827 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 126 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 124 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 178 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 177 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 179 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n15,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
rank: 176 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 165 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 166 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 167 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 164 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 82 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,829 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,830 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,830 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 81 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 119 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 83 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 118 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 116 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
rank: 80 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,830 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
rank: 117 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 207 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c09r3n19,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,830 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,831 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,830 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

rank: 205 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 206 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 204 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,831 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,831 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,830 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,831 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,831 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n04,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,831 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,832 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c09r4n09,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,831 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
rank: 139 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,832 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 89 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 122 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,832 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
rank: 137 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 90 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 123 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,832 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
rank: 136 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 91 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
rank: 121 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
rank: 138 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
rank: 88 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
rank: 120 world size: 256 addr: c05r4n07  port: 9912
07/12/2024 15:55:56 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,832 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n18,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-56_c08r2n06,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
07/12/2024 15:55:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/work/home/acehekbmzh/cxx/belle_own/train/configs/deepspeed_config_stage3_llama.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
dont_know_answers=/work/home/acehekbmzh/cxx/belle_own/work_dirs/dont_know_answers.json,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60000000000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,833 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
generation_max_length=None,
generation_num_beams=None,
gpt2_RM_train=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain/runs/Jul12_15-55-55_c08r2n14,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=steps,
lora_config=None,
lr_scheduler_type=cosine,
max_eval_samples=None,
max_grad_norm=1.0,
max_predict_samples=None,
max_source_length=1024,
max_steps=-1,
max_target_length=1024,
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,833 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
max_train_samples=None,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
negative=0.2,
net3=False,
no_cuda=False,
num_beams=None,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
overwrite_output_dir=False,
pad_to_max_length=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
pretrain_or_not=True,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/work/home/acehekbmzh/cxx/belle_own/work_dirs/20240703_llama_pretrain,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=1234,
sharded_ddp=[],
skip_memory_metrics=True,
source_prefix=,
tf32=None,
torch_compile=False,
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,834 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_debug_config=False,
use_doc=False,
use_int8_training=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=False,
use_mps_device=False,
val_max_target_length=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=1e-05,
xpu_backend=None,
)
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,834 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,835 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,834 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,835 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,836 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,836 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,836 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,837 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,838 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,839 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,839 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,839 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,839 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,839 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,840 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,840 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,840 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,840 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,840 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,840 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,841 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,841 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,841 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,841 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,841 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,841 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,839 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,841 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,841 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,842 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,842 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,842 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,843 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,843 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,844 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,844 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,843 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:666] 2024-07-12 15:55:56,844 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/config.json
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,844 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,845 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,845 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|configuration_utils.py:720] 2024-07-12 15:55:56,845 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 40076
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,845 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,845 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,845 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,846 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,845 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,843 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:2531] 2024-07-12 15:55:56,846 >> loading weights file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,846 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,846 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,846 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,846 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,847 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,847 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,846 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,848 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,847 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,848 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:2623] 2024-07-12 15:55:56,848 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,848 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,848 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,848 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,848 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,849 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,850 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,850 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,850 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,852 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,853 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,852 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,855 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,857 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,858 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,858 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,858 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,859 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,859 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,859 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,859 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,861 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,862 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,863 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,863 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,865 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,866 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,865 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,867 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,867 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,867 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,870 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:55:56,871 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

I0712 15:56:00.025197 17131 ProcessGroupNCCL.cpp:1274] NCCL_DEBUG: N/A
[2024-07-12 15:56:09,224] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 6.80B parameters

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,503 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,503 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,518 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,518 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,523 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,523 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,523 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,523 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,523 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,523 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,523 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,524 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.62s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,525 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,525 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,530 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,530 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,531 >> loading file tokenizer.model
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,531 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,531 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,531 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,531 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,531 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,531 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,532 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,533 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file added_tokens.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,533 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file special_tokens_map.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,534 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,534 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,534 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,534 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,534 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,535 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,535 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,535 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,535 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,536 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,536 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.60s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,536 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,537 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,536 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,537 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,537 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.62s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,537 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,538 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,537 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,537 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,537 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,538 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,538 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,538 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,536 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,536 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,540 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,540 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,541 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,540 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,541 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,541 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,541 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,541 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,541 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,541 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,541 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,542 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,542 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,542 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,542 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,537 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,542 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,542 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,542 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,542 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,542 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,542 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,543 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,543 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,543 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,543 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,543 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,543 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,543 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer.model
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,544 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,543 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,544 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,543 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,544 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,544 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,544 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,544 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,544 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,544 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,544 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,544 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,544 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,544 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,545 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,546 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,546 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,546 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,546 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,547 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,547 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,547 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,547 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,547 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,547 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,545 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,548 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,547 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,548 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,546 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,548 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,548 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,548 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,548 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,548 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,548 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,548 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,548 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,549 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,548 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,548 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,549 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,549 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,548 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,549 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,550 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,550 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,550 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,550 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,550 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,550 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,548 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,549 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,554 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,554 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,555 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,555 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,555 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,555 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,555 >> loading file tokenizer.model
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,556 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,555 >> loading file added_tokens.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,556 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,555 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,555 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,555 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,556 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,555 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,556 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,555 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,556 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,556 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,556 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,556 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,556 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,555 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,555 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,556 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,556 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,556 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,556 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,556 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,557 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,556 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,556 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file added_tokens.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,556 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,557 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file special_tokens_map.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,557 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,556 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,557 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,557 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,558 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,557 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,558 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,558 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,558 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,559 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,559 >> loading file added_tokens.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,559 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,559 >> loading file special_tokens_map.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,559 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,559 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,562 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,562 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,563 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,562 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,562 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,562 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,563 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,563 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,562 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,563 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer.model

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,563 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file added_tokens.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,563 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,564 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,563 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,564 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,564 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,564 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,564 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,564 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,564 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,564 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file tokenizer.model
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,565 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file added_tokens.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,564 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,565 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,565 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,565 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,565 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,565 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,565 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,565 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,565 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,565 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,565 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,565 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,566 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,566 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,566 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,566 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,566 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,567 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file tokenizer.model

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,567 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file added_tokens.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,567 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,567 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,567 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,567 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,568 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,568 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,568 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,568 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,570 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,571 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,570 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,570 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,571 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,571 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,571 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,571 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,571 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,571 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,571 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,571 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,572 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,572 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,571 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,572 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,572 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,572 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,572 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,572 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,572 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file tokenizer.model
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,572 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,572 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,572 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,573 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,573 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,573 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,573 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,573 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,573 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,573 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,573 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,573 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,573 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file tokenizer.model
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,573 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file added_tokens.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,573 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,573 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,574 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,574 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file special_tokens_map.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,573 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,573 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,574 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,573 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,573 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,574 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,574 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,574 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,574 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,574 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,574 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,575 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,574 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,575 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,575 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,576 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,576 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,576 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,576 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,578 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,578 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,578 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,579 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,579 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,579 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,579 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,579 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,579 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file special_tokens_map.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,580 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file added_tokens.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,581 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,580 >> loading file added_tokens.json
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,580 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer.model
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,581 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,581 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,582 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,582 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,582 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,582 >> loading file tokenizer_config.json

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
[INFO|modeling_utils.py:3190] 2024-07-12 15:57:14,582 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-07-12 15:57:14,582 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:535] 2024-07-12 15:57:14,589 >> loading configuration file /work/home/acehekbmzh/data/hf_home/linly_chinese_llama_7b_hf/generation_config.json
[INFO|configuration_utils.py:575] 2024-07-12 15:57:14,589 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.1"
}

[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,590 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,590 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,590 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1807] 2024-07-12 15:57:14,590 >> loading file tokenizer_config.json
RAM memory % used: 18.4
RAM Used (GB): 23.68028672
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 16.4
RAM Used (GB): 21.011902464
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.006482944
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 28.2
RAM Used (GB): 37.056794624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.6
RAM Used (GB): 22.957236224
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.845136896
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 14.6
RAM Used (GB): 18.773397504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 24.6
RAM Used (GB): 32.156196864
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.761238528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.006482944
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.404511232
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 25.4
RAM Used (GB): 33.500090368
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.9
RAM Used (GB): 23.31348992
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.845136896
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.761238528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.6
RAM Used (GB): 22.957236224
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 26.9
RAM Used (GB): 35.312992256
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 16.4
RAM Used (GB): 21.243072512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 14.6
RAM Used (GB): 18.773397504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 28.2
RAM Used (GB): 37.056794624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 24.6
RAM Used (GB): 32.156196864
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 23.0
RAM Used (GB): 29.968637952
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 16.4
RAM Used (GB): 21.243072512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.36661504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.60s/it]
RAM memory % used: 23.5
RAM Used (GB): 30.786977792
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.8
RAM Used (GB): 26.993201152
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 24.6
RAM Used (GB): 32.156196864
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.395139584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 29.2
RAM Used (GB): 38.487478272
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 28.7
RAM Used (GB): 37.816086528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.0
RAM Used (GB): 25.90304256
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 21.8
RAM Used (GB): 28.342730752
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.786510848
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.8236288
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.845136896
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 19.9
RAM Used (GB): 25.913614336
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.231299584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 11.6
RAM Used (GB): 14.864662528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.72567552
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 24.8
RAM Used (GB): 32.531083264
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 17.3
RAM Used (GB): 22.359240704
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.348342784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 28.9
RAM Used (GB): 38.095536128
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 16.8
RAM Used (GB): 21.630763008
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 23.0
RAM Used (GB): 29.968637952
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 11.7
RAM Used (GB): 14.945792
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 19.6
RAM Used (GB): 25.317330944
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.734293504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.3
RAM Used (GB): 22.471614464
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.26642688
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 26.9
RAM Used (GB): 35.312992256
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.873240064
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 18.0
RAM Used (GB): 23.404511232
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 17.5
RAM Used (GB): 22.581936128
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.9
RAM Used (GB): 23.31348992
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.291177472
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 24.8
RAM Used (GB): 32.531083264
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.348342784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 29.3
RAM Used (GB): 38.554394624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.836793344
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 13.5
RAM Used (GB): 17.351278592
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 20.1
RAM Used (GB): 26.028224512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 24.6
RAM Used (GB): 32.156196864
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 20.0
RAM Used (GB): 25.90304256
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 16.4
RAM Used (GB): 21.243072512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 23.5
RAM Used (GB): 30.786977792
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 16.4
RAM Used (GB): 21.011902464
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 21.3
RAM Used (GB): 27.744903168
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.395139584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 21.9
RAM Used (GB): 28.423036928
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.816086528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 12.2
RAM Used (GB): 15.71190784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.36661504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.60s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.786510848
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.2
RAM Used (GB): 26.288275456
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 20.8
RAM Used (GB): 26.993201152
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 22.6
RAM Used (GB): 29.51141376
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.231299584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 24.8
RAM Used (GB): 32.531083264
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.3
RAM Used (GB): 22.359240704
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 29.3
RAM Used (GB): 38.554394624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.836793344
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 13.5
RAM Used (GB): 17.351278592
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.734293504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 20.0
RAM Used (GB): 25.90304256
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 18.4
RAM Used (GB): 23.68028672
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 19.9
RAM Used (GB): 25.873240064
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 21.8
RAM Used (GB): 28.342730752
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 18.0
RAM Used (GB): 23.404511232
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 17.5
RAM Used (GB): 22.581936128
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.31348992
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 11.6
RAM Used (GB): 14.864662528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 19.6
RAM Used (GB): 25.286664192
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 28.2
RAM Used (GB): 37.056794624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.62s/it]
RAM memory % used: 14.8
RAM Used (GB): 18.914664448
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 28.8
RAM Used (GB): 37.953695744
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 20.0
RAM Used (GB): 26.002952192
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 21.7
RAM Used (GB): 28.28972032
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 22.5
RAM Used (GB): 29.37573376
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 23.0
RAM Used (GB): 29.968637952
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 24.4
RAM Used (GB): 32.120127488
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.40s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 12.2
RAM Used (GB): 15.71190784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 16.4
RAM Used (GB): 21.243072512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 25.4
RAM Used (GB): 33.500090368
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.62s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.291177472
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 21.3
RAM Used (GB): 27.744903168
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 18.0
RAM Used (GB): 23.348342784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 16.8
RAM Used (GB): 21.630763008
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.1
RAM Used (GB): 26.028224512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 29.3
RAM Used (GB): 38.554394624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.873240064
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.36661504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.60s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.31348992
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.38s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 29.2
RAM Used (GB): 38.487478272
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 13.5
RAM Used (GB): 17.351278592
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 23.0
RAM Used (GB): 29.968637952
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 21.8
RAM Used (GB): 28.342730752
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.786510848
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 11.6
RAM Used (GB): 14.8236288
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 19.9
RAM Used (GB): 25.913614336
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 22.6
RAM Used (GB): 29.51141376
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.291177472
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.231299584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.72567552
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.3
RAM Used (GB): 22.359240704
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 28.9
RAM Used (GB): 38.095536128
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 17.4
RAM Used (GB): 22.523101184
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 11.7
RAM Used (GB): 14.945792
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.836793344
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.002952192
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 18.4
RAM Used (GB): 23.68028672
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 24.4
RAM Used (GB): 32.120127488
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 12.2
RAM Used (GB): 15.71190784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 20.0
RAM Used (GB): 25.90304256
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 19.2
RAM Used (GB): 24.874868736
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 16.4
RAM Used (GB): 21.011902464
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.291177472
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 21.3
RAM Used (GB): 27.744903168
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.348342784
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 28.8
RAM Used (GB): 37.953695744
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.002952192
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.1
RAM Used (GB): 26.028224512
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.26642688
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.36661504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 23.5
RAM Used (GB): 30.786977792
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.395139584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 14.6
RAM Used (GB): 18.773397504
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 11.7
RAM Used (GB): 14.945792
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 29.2
RAM Used (GB): 38.487478272
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 21.9
RAM Used (GB): 28.423036928
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 29.3
RAM Used (GB): 38.554394624
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 21.8
RAM Used (GB): 28.342730752
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.786510848
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.8236288
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 17.5
RAM Used (GB): 22.581936128
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.864662528
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.231299584
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.72567552
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.006482944
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.4
RAM Used (GB): 22.523101184
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 25.7
RAM Used (GB): 33.765490688
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 13.5
RAM Used (GB): 17.351278592
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 16.9
RAM Used (GB): 21.75377408
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 25.7
RAM Used (GB): 33.765490688
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 19.2
RAM Used (GB): 24.874868736
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 20.2
RAM Used (GB): 26.288275456
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 19.6
RAM Used (GB): 25.286664192
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 16.8
RAM Used (GB): 21.630763008
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.002952192
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 17.3
RAM Used (GB): 22.471614464
07/12/2024 15:57:19 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.957236224
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 19.2
RAM Used (GB): 24.874868736
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.913614336
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 18.1
RAM Used (GB): 23.395139584
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 29.2
RAM Used (GB): 38.487478272
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 21.9
RAM Used (GB): 28.423036928
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 18.0
RAM Used (GB): 23.404511232
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 16.4
RAM Used (GB): 21.011902464
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.816086528
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 20.1
RAM Used (GB): 25.96313088
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 14.8
RAM Used (GB): 18.914664448
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 28.9
RAM Used (GB): 38.095536128
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 25.7
RAM Used (GB): 33.765490688
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 22.5
RAM Used (GB): 29.37573376
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 19.6
RAM Used (GB): 25.317330944
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.734293504
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.75377408
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 19.2
RAM Used (GB): 24.874868736
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 17.4
RAM Used (GB): 22.523101184
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 22.6
RAM Used (GB): 29.51141376
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.913614336
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 17.5
RAM Used (GB): 22.581936128
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 26.9
RAM Used (GB): 35.312992256
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 17.3
RAM Used (GB): 22.471614464
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 11.7
RAM Used (GB): 14.945792
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.957236224
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.873240064
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 28.8
RAM Used (GB): 37.953695744
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 16.8
RAM Used (GB): 21.630763008
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 18.0
RAM Used (GB): 23.26642688
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 14.6
RAM Used (GB): 18.773397504
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 17.4
RAM Used (GB): 22.523101184
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 20.1
RAM Used (GB): 26.028224512
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.75377408
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 20.8
RAM Used (GB): 26.993201152
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 14.8
RAM Used (GB): 18.914664448
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.3
RAM Used (GB): 22.359240704
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 19.6
RAM Used (GB): 25.317330944
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 25.4
RAM Used (GB): 33.500090368
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.62s/it]
RAM memory % used: 20.2
RAM Used (GB): 26.288275456
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.77638144
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 17.3
RAM Used (GB): 22.471614464
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 12.2
RAM Used (GB): 15.71190784
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 11.6
RAM Used (GB): 14.77638144
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.1
RAM Used (GB): 25.96313088
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 21.7
RAM Used (GB): 28.28972032
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 14.8
RAM Used (GB): 18.914664448
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 22.5
RAM Used (GB): 29.37573376
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 21.7
RAM Used (GB): 28.28972032
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 21.9
RAM Used (GB): 28.423036928
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 24.8
RAM Used (GB): 32.531570688
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 20.1
RAM Used (GB): 25.96313088
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.836793344
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.8236288
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 25.7
RAM Used (GB): 33.765490688
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.816086528
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 22.6
RAM Used (GB): 29.51141376
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 28.9
RAM Used (GB): 38.095536128
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.72567552
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.59s/it]
RAM memory % used: 21.7
RAM Used (GB): 28.28972032
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.761238528
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 20.0
RAM Used (GB): 26.006482944
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 21.3
RAM Used (GB): 27.744903168
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 28.2
RAM Used (GB): 37.056794624
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.53s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 26.9
RAM Used (GB): 35.312992256
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 28.8
RAM Used (GB): 37.953695744
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 20.8
RAM Used (GB): 26.993201152
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 23.5
RAM Used (GB): 30.786977792
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 18.0
RAM Used (GB): 23.26642688
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 19.6
RAM Used (GB): 25.317330944
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 24.4
RAM Used (GB): 32.120127488
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.61s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.864662528
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 20.2
RAM Used (GB): 26.288275456
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.77638144
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 20.1
RAM Used (GB): 25.96313088
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 11.6
RAM Used (GB): 14.845136896
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]
RAM memory % used: 11.5
RAM Used (GB): 14.761238528
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 22.5
RAM Used (GB): 29.37573376
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.55s/it]
RAM memory % used: 28.7
RAM Used (GB): 37.734293504
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 18.4
RAM Used (GB): 23.68028672
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.32s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.58s/it]
RAM memory % used: 16.9
RAM Used (GB): 21.75377408
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.57s/it]
RAM memory % used: 19.6
RAM Used (GB): 25.286664192
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 25.4
RAM Used (GB): 33.500090368
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.62s/it]
RAM memory % used: 11.6
RAM Used (GB): 14.77638144
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Set the eos_token_id and bos_token_id of LLama model tokenizer
tokenizer.eos_token_id = 2
tokenizer.pad_token_id = None
tokenizer.bos_token_id = 1
RAM memory % used: 24.4
RAM Used (GB): 32.120127488
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 19.6
RAM Used (GB): 25.286664192
07/12/2024 15:57:20 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-63e058c0fe4d481a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 29.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]
RAM memory % used: 27.2
RAM Used (GB): 35.696070656
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.20s/it]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
RAM memory % used: 20.5
RAM Used (GB): 26.680827904
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.18s/it]
100%|██████████| 1/1 [03:21<00:00, 201.21s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.147732992
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.16s/it]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.734442496
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
RAM memory % used: 23.3
RAM Used (GB): 30.365396992
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
RAM memory % used: 20.2
RAM Used (GB): 26.269618176
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
RAM memory % used: 21.1
RAM Used (GB): 27.381612544
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]
100%|██████████| 1/1 [03:21<00:00, 201.73s/it]
RAM memory % used: 20.4
RAM Used (GB): 26.409750528
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.160205312
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]
RAM memory % used: 25.7
RAM Used (GB): 33.887571968
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]
RAM memory % used: 29.0
RAM Used (GB): 38.188634112
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
RAM memory % used: 21.1
RAM Used (GB): 27.381612544
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
RAM memory % used: 29.5
RAM Used (GB): 38.882942976
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
100%|██████████| 1/1 [03:21<00:00, 201.53s/it]
RAM memory % used: 18.3
RAM Used (GB): 23.614595072
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.354114048
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
RAM memory % used: 25.7
RAM Used (GB): 33.887571968
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.80s/it]
100%|██████████| 1/1 [03:21<00:00, 201.83s/it]
RAM memory % used: 17.7
RAM Used (GB): 22.925389824
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]
RAM memory % used: 21.5
RAM Used (GB): 28.129820672
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.21s/it]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
RAM memory % used: 29.5
RAM Used (GB): 38.921084928
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
RAM memory % used: 19.8
RAM Used (GB): 25.657806848
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.09s/it]
100%|██████████| 1/1 [03:21<00:00, 201.12s/it]
RAM memory % used: 23.8
RAM Used (GB): 31.174254592
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]
100%|██████████| 1/1 [03:21<00:00, 201.73s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.354114048
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.78371072
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
RAM memory % used: 20.4
RAM Used (GB): 26.409750528
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
RAM memory % used: 22.9
RAM Used (GB): 29.903429632
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
RAM memory % used: 21.5
RAM Used (GB): 28.129820672
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.53s/it]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
RAM memory % used: 23.3
RAM Used (GB): 30.365396992
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.73s/it]
100%|██████████| 1/1 [03:21<00:00, 201.76s/it]
RAM memory % used: 12.0
RAM Used (GB): 15.321690112
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.42s/it]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.734442496
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
RAM memory % used: 25.0
RAM Used (GB): 32.905641984
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]
RAM memory % used: 20.5
RAM Used (GB): 26.680827904
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.254798336
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
RAM memory % used: 23.8
RAM Used (GB): 31.174254592
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.296188928
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.147732992
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.21s/it]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
RAM memory % used: 17.2
RAM Used (GB): 22.104195072
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
RAM memory % used: 20.2
RAM Used (GB): 26.3087104
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.20037888
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]
RAM memory % used: 26.0
RAM Used (GB): 34.154504192
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.349408256
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.17s/it]
100%|██████████| 1/1 [03:21<00:00, 201.20s/it]
RAM memory % used: 19.8
RAM Used (GB): 25.657806848
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.14s/it]
100%|██████████| 1/1 [03:21<00:00, 201.17s/it]
RAM memory % used: 20.4
RAM Used (GB): 26.409750528
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.650292736
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.75s/it]
100%|██████████| 1/1 [03:21<00:00, 201.79s/it]
RAM memory % used: 24.9
RAM Used (GB): 32.561954816
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.82s/it]
100%|██████████| 1/1 [03:21<00:00, 201.85s/it]
RAM memory % used: 24.7
RAM Used (GB): 32.504164352
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
RAM memory % used: 23.3
RAM Used (GB): 30.365396992
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]
RAM memory % used: 18.6
RAM Used (GB): 24.059510784
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.858285056
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.3995392
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.650292736
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.734442496
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.764754432
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.71s/it]
100%|██████████| 1/1 [03:21<00:00, 201.74s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.20037888
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
RAM memory % used: 17.9
RAM Used (GB): 23.354114048
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.83s/it]
100%|██████████| 1/1 [03:21<00:00, 201.87s/it]
RAM memory % used: 26.0
RAM Used (GB): 34.154504192
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
100%|██████████| 1/1 [03:21<00:00, 201.26s/it]
RAM memory % used: 27.2
RAM Used (GB): 35.696070656
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.76s/it]
100%|██████████| 1/1 [03:21<00:00, 201.79s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.764754432
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
RAM memory % used: 13.8
RAM Used (GB): 17.726398464
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.78371072
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.80s/it]
100%|██████████| 1/1 [03:21<00:00, 201.83s/it]
RAM memory % used: 20.2
RAM Used (GB): 26.269618176
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]
RAM memory % used: 17.8
RAM Used (GB): 22.953590784
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]
RAM memory % used: 22.2
RAM Used (GB): 28.798898176
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.42s/it]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
RAM memory % used: 29.2
RAM Used (GB): 38.480285696
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.237873664
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.79s/it]
100%|██████████| 1/1 [03:21<00:00, 201.82s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.226761216
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.50s/it]
RAM memory % used: 17.0
RAM Used (GB): 22.010568704
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.706663936
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.383876096
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.84s/it]
100%|██████████| 1/1 [03:21<00:00, 201.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 301.97it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 430.41it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 438.00it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 408.48it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 409.76it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 495.08it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 355.15it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 313.97it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 398.17it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 508.71it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 504.79it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 224.71it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 499.44it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 392.98it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 427.42it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 509.88it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 495.60it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 337.14it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 279.92it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.01it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 203.37it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 329.90it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 87.01it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 175.26it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 495.60it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 503.28it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 237.84it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 499.68it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 271.69it/s]
RAM memory % used: 29.5
RAM Used (GB): 38.882942976
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 508.52it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 62.61it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 438.46it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 419.56it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 516.67it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 389.77it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 260.73it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 431.60it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 318.40it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 506.19it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 404.93it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 422.81it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 475.17it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 36.97it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 220.61it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 512.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 344.02it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 155.57it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 408.80it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 439.42it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]
100%|██████████| 1/1 [03:21<00:00, 201.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 177.92it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 283.48it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 419.77it/s]
RAM memory % used: 26.0
RAM Used (GB): 34.154504192
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]
100%|██████████| 1/1 [03:21<00:00, 201.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 496.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 512.63it/s]
RAM memory % used: 22.8
RAM Used (GB): 29.771874304
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 199.26it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]
100%|██████████| 1/1 [03:21<00:00, 201.35s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.698161664
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 512.75it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.254798336
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.296188928
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]
RAM memory % used: 26.0
RAM Used (GB): 34.154504192
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 513.00it/s]
RAM memory % used: 29.5
RAM Used (GB): 38.921084928
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 445.73it/s]
RAM memory % used: 25.0
RAM Used (GB): 32.905641984
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
RAM memory % used: 18.3
RAM Used (GB): 23.614595072
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 525.47it/s]
RAM memory % used: 22.2
RAM Used (GB): 28.798898176
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
RAM memory % used: 22.9
RAM Used (GB): 29.903429632
07/12/2024 16:00:41 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.734442496
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.383876096
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 385.90it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 403.18it/s]
RAM memory % used: 18.3
RAM Used (GB): 23.740219392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 419.56it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 439.10it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 440.39it/s]
RAM memory % used: 19.5
RAM Used (GB): 25.25628416
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
RAM memory % used: 28.9
RAM Used (GB): 38.113071104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.16s/it]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.399052288
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.84s/it]
100%|██████████| 1/1 [03:21<00:00, 201.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.71it/s]
RAM memory % used: 22.9
RAM Used (GB): 29.903429632
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 431.29it/s]
RAM memory % used: 12.0
RAM Used (GB): 15.321690112
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 125.26it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 496.90it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 125.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 459.05it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 513.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 195.28it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 338.33it/s]
RAM memory % used: 17.0
RAM Used (GB): 22.010568704
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 523.18it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 502.79it/s]
RAM memory % used: 24.7
RAM Used (GB): 32.504164352
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.12s/it]
100%|██████████| 1/1 [03:21<00:00, 201.15s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.78371072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.156744192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 483.38it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 505.46it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 299.91it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 262.00it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 465.41it/s]
RAM memory % used: 17.6
RAM Used (GB): 22.858285056
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.26s/it]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 516.29it/s]
RAM memory % used: 20.2
RAM Used (GB): 26.3087104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
RAM memory % used: 22.0
RAM Used (GB): 28.671115264
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
RAM memory % used: 17.1
RAM Used (GB): 22.13931008
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.14s/it]
100%|██████████| 1/1 [03:21<00:00, 201.18s/it]
RAM memory % used: 29.2
RAM Used (GB): 38.480285696
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]
RAM memory % used: 18.6
RAM Used (GB): 24.059510784
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.84s/it]
100%|██████████| 1/1 [03:21<00:00, 201.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 496.43it/s]
RAM memory % used: 17.2
RAM Used (GB): 22.104195072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.336958464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.20s/it]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.160205312
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.13s/it]
100%|██████████| 1/1 [03:21<00:00, 201.16s/it]
RAM memory % used: 28.5
RAM Used (GB): 37.445316608
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.74s/it]
100%|██████████| 1/1 [03:21<00:00, 201.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 319.30it/s]
RAM memory % used: 11.9
RAM Used (GB): 15.254798336
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.41s/it]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]
RAM memory % used: 18.3
RAM Used (GB): 23.740219392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 284.48it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 503.94it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 409.64it/s]
RAM memory % used: 12.0
RAM Used (GB): 15.321690112
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.698161664
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.50s/it]
100%|██████████| 1/1 [03:21<00:00, 201.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 168.83it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.3995392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
RAM memory % used: 17.8
RAM Used (GB): 22.953590784
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 428.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
RAM memory % used: 17.7
RAM Used (GB): 22.925389824
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
100%|██████████| 1/1 [03:21<00:00, 201.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 110.36it/s]
RAM memory % used: 20.2
RAM Used (GB): 26.269618176
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 513.82it/s]
RAM memory % used: 20.2
RAM Used (GB): 26.3087104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.399052288
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]
RAM memory % used: 18.3
RAM Used (GB): 23.614595072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 511.56it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 87.66it/s]
RAM memory % used: 12.0
RAM Used (GB): 15.321690112
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
100%|██████████| 1/1 [03:21<00:00, 201.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 289.02it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.764754432
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 513.19it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 504.06it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 498.61it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 88.54it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 409.24it/s]
RAM memory % used: 29.2
RAM Used (GB): 38.480285696
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
100%|██████████| 1/1 [03:21<00:00, 201.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 519.03it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 293.62it/s]
RAM memory % used: 15.1
RAM Used (GB): 19.307737088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]
RAM memory % used: 17.2
RAM Used (GB): 22.104195072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
RAM memory % used: 28.5
RAM Used (GB): 37.445316608
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
RAM memory % used: 24.7
RAM Used (GB): 32.504164352
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
RAM memory % used: 22.0
RAM Used (GB): 28.671115264
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 491.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 428.82it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 507.85it/s]
RAM memory % used: 29.4
RAM Used (GB): 38.68121088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
RAM memory % used: 14.8
RAM Used (GB): 19.167358976
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.74s/it]
100%|██████████| 1/1 [03:21<00:00, 201.77s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.336958464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 202.01it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 529.25it/s]
RAM memory % used: 22.2
RAM Used (GB): 28.798898176
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.858285056
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.78it/s]
RAM memory % used: 17.1
RAM Used (GB): 22.13931008
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 517.43it/s]
RAM memory % used: 21.5
RAM Used (GB): 28.129820672
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 258.30it/s]
RAM memory % used: 11.9
RAM Used (GB): 15.254798336
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 497.60it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.18s/it]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 519.48it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 516.86it/s]
RAM memory % used: 25.7
RAM Used (GB): 33.887571968
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 489.36it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.53s/it]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
RAM memory % used: 24.9
RAM Used (GB): 32.561954816
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]
100%|██████████| 1/1 [03:21<00:00, 201.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 485.79it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 526.26it/s]
RAM memory % used: 18.3
RAM Used (GB): 23.614595072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.41s/it]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 498.43it/s]
RAM memory % used: 14.8
RAM Used (GB): 19.167358976
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.83s/it]
100%|██████████| 1/1 [03:21<00:00, 201.86s/it]
RAM memory % used: 12.5
RAM Used (GB): 16.114184192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.156744192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.41s/it]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]
RAM memory % used: 29.0
RAM Used (GB): 38.188634112
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.296188928
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]
RAM memory % used: 17.7
RAM Used (GB): 22.925389824
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 526.66it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.383876096
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.21s/it]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
RAM memory % used: 29.5
RAM Used (GB): 38.882942976
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
100%|██████████| 1/1 [03:21<00:00, 201.71s/it]
RAM memory % used: 19.9
RAM Used (GB): 25.706663936
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 476.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]
100%|██████████| 1/1 [03:21<00:00, 201.35s/it]
RAM memory % used: 28.9
RAM Used (GB): 38.113071104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
100%|██████████| 1/1 [03:21<00:00, 201.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 530.99it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 501.29it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 287.87it/s]
RAM memory % used: 17.0
RAM Used (GB): 22.010568704
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
RAM memory % used: 17.1
RAM Used (GB): 22.13931008
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.764754432
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
RAM memory % used: 24.9
RAM Used (GB): 32.561954816
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
RAM memory % used: 22.9
RAM Used (GB): 29.903429632
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 508.65it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 304.07it/s]
RAM memory % used: 18.4
RAM Used (GB): 23.785496576
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 525.60it/s]
RAM memory % used: 11.9
RAM Used (GB): 15.20037888
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 517.82it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]
RAM memory % used: 13.8
RAM Used (GB): 17.726398464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 429.04it/s]
RAM memory % used: 20.5
RAM Used (GB): 26.680827904
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 485.79it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
100%|██████████| 1/1 [03:21<00:00, 201.42s/it]
RAM memory % used: 18.2
RAM Used (GB): 23.698161664
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 488.96it/s]
RAM memory % used: 19.8
RAM Used (GB): 25.657806848
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 329.66it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.349408256
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 117.30it/s]
RAM memory % used: 24.9
RAM Used (GB): 32.561954816
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.73s/it]
100%|██████████| 1/1 [03:21<00:00, 201.77s/it]
RAM memory % used: 18.6
RAM Used (GB): 24.059510784
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 409.28it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 520.19it/s]
RAM memory % used: 12.5
RAM Used (GB): 16.114184192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
RAM memory % used: 25.7
RAM Used (GB): 33.887571968
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.13s/it]
100%|██████████| 1/1 [03:21<00:00, 201.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 491.19it/s]
RAM memory % used: 17.8
RAM Used (GB): 22.953590784
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
RAM memory % used: 20.4
RAM Used (GB): 26.409750528
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
RAM memory % used: 14.8
RAM Used (GB): 19.167358976
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 488.56it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
RAM memory % used: 22.1
RAM Used (GB): 28.745519104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 494.44it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 469.53it/s]
RAM memory % used: 16.7
RAM Used (GB): 21.650292736
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 463.46it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
RAM memory % used: 22.0
RAM Used (GB): 28.671115264
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 491.02it/s]
RAM memory % used: 29.4
RAM Used (GB): 38.68121088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 490.45it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.53s/it]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
RAM memory % used: 19.8
RAM Used (GB): 25.657806848
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
RAM memory % used: 23.8
RAM Used (GB): 31.174254592
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 417.93it/s]
RAM memory % used: 19.9
RAM Used (GB): 25.706663936
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 460.91it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 502.85it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 183.51it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.632273408
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]
100%|██████████| 1/1 [03:21<00:00, 201.47s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.237873664
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.16s/it]
100%|██████████| 1/1 [03:21<00:00, 201.20s/it]
RAM memory % used: 27.2
RAM Used (GB): 35.696070656
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 436.13it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
RAM memory % used: 29.0
RAM Used (GB): 38.188634112
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 425.08it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
100%|██████████| 1/1 [03:21<00:00, 201.71s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.147732992
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.78s/it]
100%|██████████| 1/1 [03:21<00:00, 201.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 482.38it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 519.87it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.632273408
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]
100%|██████████| 1/1 [03:21<00:00, 201.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 461.17it/s]
RAM memory % used: 19.9
RAM Used (GB): 25.706663936
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.20s/it]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.20037888
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.41s/it]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.237873664
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.83s/it]
100%|██████████| 1/1 [03:21<00:00, 201.86s/it]
RAM memory % used: 13.8
RAM Used (GB): 17.726398464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 476.41it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 205.55it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.07it/s]
RAM memory % used: 17.0
RAM Used (GB): 22.010568704
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 499.08it/s]
RAM memory % used: 18.4
RAM Used (GB): 23.785496576
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 499.08it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.35s/it]
100%|██████████| 1/1 [03:21<00:00, 201.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 452.31it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.349408256
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 501.41it/s]
RAM memory % used: 15.1
RAM Used (GB): 19.307737088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 502.19it/s]
RAM memory % used: 18.6
RAM Used (GB): 24.059510784
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 251.76it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.15s/it]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 416.85it/s]
RAM memory % used: 22.1
RAM Used (GB): 28.745519104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 421.24it/s]
RAM memory % used: 20.2
RAM Used (GB): 26.269618176
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.51s/it]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
RAM memory % used: 18.3
RAM Used (GB): 23.740219392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 477.33it/s]
RAM memory % used: 29.5
RAM Used (GB): 38.921084928
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.42s/it]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
RAM memory % used: 17.8
RAM Used (GB): 22.953590784
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 492.17it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.41s/it]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 517.88it/s]
RAM memory % used: 22.8
RAM Used (GB): 29.771874304
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.16s/it]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]
RAM memory % used: 29.1
RAM Used (GB): 38.336958464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
RAM memory % used: 29.4
RAM Used (GB): 38.68121088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 475.22it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
RAM memory % used: 23.8
RAM Used (GB): 31.174254592
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 497.01it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]
RAM memory % used: 15.1
RAM Used (GB): 19.307737088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 509.39it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 507.29it/s]
RAM memory % used: 25.0
RAM Used (GB): 32.905641984
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
RAM memory % used: 21.5
RAM Used (GB): 28.129820672
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 526.72it/s]
RAM memory % used: 19.5
RAM Used (GB): 25.25628416
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
100%|██████████| 1/1 [03:21<00:00, 201.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 452.61it/s]
RAM memory % used: 11.8
RAM Used (GB): 15.147732992
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.82s/it]
100%|██████████| 1/1 [03:21<00:00, 201.85s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.399052288
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]
RAM memory % used: 29.0
RAM Used (GB): 38.188634112
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 522.85it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 376.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]
RAM memory % used: 17.2
RAM Used (GB): 22.104195072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]
100%|██████████| 1/1 [03:21<00:00, 201.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 511.25it/s]
RAM memory % used: 29.1
RAM Used (GB): 38.336958464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.3995392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 461.47it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.55s/it]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
RAM memory % used: 11.9
RAM Used (GB): 15.226761216
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 490.73it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
RAM memory % used: 21.1
RAM Used (GB): 27.381612544
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
RAM memory % used: 28.5
RAM Used (GB): 37.445316608
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 457.74it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.21s/it]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
RAM memory % used: 12.5
RAM Used (GB): 16.114184192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 478.80it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.26s/it]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 529.45it/s]
RAM memory % used: 22.2
RAM Used (GB): 28.798898176
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 473.56it/s]
RAM memory % used: 22.8
RAM Used (GB): 29.771874304
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 425.21it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.296188928
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
100%|██████████| 1/1 [03:21<00:00, 201.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 178.74it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 476.73it/s]
RAM memory % used: 11.9
RAM Used (GB): 15.226761216
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
RAM memory % used: 14.8
RAM Used (GB): 19.167358976
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 498.14it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.650292736
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 426.86it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.71s/it]
100%|██████████| 1/1 [03:21<00:00, 201.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 515.90it/s]
RAM memory % used: 19.5
RAM Used (GB): 25.25628416
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]
100%|██████████| 1/1 [03:21<00:00, 201.39s/it]
RAM memory % used: 18.4
RAM Used (GB): 23.785496576
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 505.70it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]
100%|██████████| 1/1 [03:21<00:00, 201.72s/it]
RAM memory % used: 28.9
RAM Used (GB): 38.113071104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 467.96it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
RAM memory % used: 16.7
RAM Used (GB): 21.399052288
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.156744192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 426.55it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 510.69it/s]
RAM memory % used: 29.2
RAM Used (GB): 38.480285696
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 522.52it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.22s/it]
100%|██████████| 1/1 [03:21<00:00, 201.25s/it]
RAM memory % used: 20.5
RAM Used (GB): 26.680827904
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.26s/it]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 472.86it/s]
RAM memory % used: 22.0
RAM Used (GB): 28.671115264
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.24s/it]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 499.14it/s]
RAM memory % used: 12.5
RAM Used (GB): 16.114184192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 506.31it/s]
RAM memory % used: 19.5
RAM Used (GB): 25.25628416
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 505.76it/s]
RAM memory % used: 27.2
RAM Used (GB): 35.696070656
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.156744192
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.84it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.48s/it]
100%|██████████| 1/1 [03:21<00:00, 201.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 517.11it/s]
RAM memory % used: 18.4
RAM Used (GB): 23.785496576
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]
100%|██████████| 1/1 [03:21<00:00, 201.46s/it]
RAM memory % used: 11.8
RAM Used (GB): 15.160205312
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 515.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.26s/it]
100%|██████████| 1/1 [03:21<00:00, 201.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 486.63it/s]
RAM memory % used: 22.1
RAM Used (GB): 28.745519104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 506.31it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.41s/it]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
RAM memory % used: 28.5
RAM Used (GB): 37.445316608
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.84s/it]
100%|██████████| 1/1 [03:21<00:00, 201.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 458.29it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 409.56it/s]
RAM memory % used: 25.0
RAM Used (GB): 32.905641984
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 461.72it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 483.16it/s]
RAM memory % used: 22.8
RAM Used (GB): 29.771874304
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
RAM memory % used: 20.3
RAM Used (GB): 26.383876096
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.81s/it]
100%|██████████| 1/1 [03:21<00:00, 201.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 515.52it/s]
RAM memory % used: 20.2
RAM Used (GB): 26.3087104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
100%|██████████| 1/1 [03:21<00:00, 201.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 511.63it/s]
RAM memory % used: 15.1
RAM Used (GB): 19.307737088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 467.28it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.27s/it]
100%|██████████| 1/1 [03:21<00:00, 201.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 479.84it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 496.90it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.78371072
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.56s/it]
100%|██████████| 1/1 [03:21<00:00, 201.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 401.71it/s]
RAM memory % used: 17.7
RAM Used (GB): 22.925389824
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.28s/it]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 507.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.26it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.3995392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 465.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.37s/it]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 477.98it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.698161664
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.79s/it]
100%|██████████| 1/1 [03:21<00:00, 201.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 509.82it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 485.51it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.632273408
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.20s/it]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 508.40it/s]
RAM memory % used: 11.9
RAM Used (GB): 15.226761216
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 488.51it/s]
RAM memory % used: 29.5
RAM Used (GB): 38.882942976
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 496.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.42s/it]
100%|██████████| 1/1 [03:21<00:00, 201.45s/it]
RAM memory % used: 13.8
RAM Used (GB): 17.726398464
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 467.18it/s]
RAM memory % used: 20.3
RAM Used (GB): 26.349408256
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.40s/it]
100%|██████████| 1/1 [03:21<00:00, 201.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 500.04it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.33s/it]
100%|██████████| 1/1 [03:21<00:00, 201.36s/it]
RAM memory % used: 21.1
RAM Used (GB): 27.381612544
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.19s/it]
100%|██████████| 1/1 [03:21<00:00, 201.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 224.81it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 514.45it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 497.25it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 413.31it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 516.09it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 516.92it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 488.96it/s]
RAM memory % used: 18.2
RAM Used (GB): 23.632273408
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 503.34it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 518.39it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 472.33it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 495.43it/s]
RAM memory % used: 23.3
RAM Used (GB): 30.365396992
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.65s/it]
100%|██████████| 1/1 [03:21<00:00, 201.68s/it]
RAM memory % used: 17.6
RAM Used (GB): 22.858285056
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.64s/it]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 520.00it/s]
RAM memory % used: 11.8
RAM Used (GB): 15.160205312
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 505.16it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 502.73it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.18s/it]
100%|██████████| 1/1 [03:21<00:00, 201.21s/it]
RAM memory % used: 24.7
RAM Used (GB): 32.504164352
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 514.83it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.54s/it]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 497.37it/s]
RAM memory % used: 22.1
RAM Used (GB): 28.745519104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.67s/it]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]
RAM memory % used: 28.9
RAM Used (GB): 38.113071104
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.57s/it]
100%|██████████| 1/1 [03:21<00:00, 201.60s/it]
RAM memory % used: 29.5
RAM Used (GB): 38.921084928
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 413.56it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.58s/it]
100%|██████████| 1/1 [03:21<00:00, 201.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 501.11it/s]
RAM memory % used: 11.9
RAM Used (GB): 15.237873664
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
100%|██████████| 1/1 [03:21<00:00, 201.70s/it]
RAM memory % used: 29.4
RAM Used (GB): 38.68121088
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 526.20it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.63s/it]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 511.31it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 475.92it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 462.28it/s]
RAM memory % used: 18.3
RAM Used (GB): 23.740219392
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.62s/it]
100%|██████████| 1/1 [03:21<00:00, 201.66s/it]
RAM memory % used: 17.1
RAM Used (GB): 22.13931008
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.31s/it]
100%|██████████| 1/1 [03:21<00:00, 201.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 473.40it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 424.44it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 414.78it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 496.72it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 486.47it/s]
RAM memory % used: 17.9
RAM Used (GB): 23.354114048
07/12/2024 16:00:42 - WARNING - datasets.builder - Found cached dataset json (/work/home/acehekbmzh/cxx/hf_power_cache_dir/json/default-aa787ffc38792a8e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [03:21<00:00, 201.76s/it]
100%|██████████| 1/1 [03:21<00:00, 201.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 492.87it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 511.06it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 527.39it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 524.29it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 461.67it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 495.08it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 525.60it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 513.57it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 494.67it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 506.56it/s]

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 435.82it/s]
num_gpus = 4, training_nums = 19808859, t_total = 19345.0, warmup_steps = 967, eval_steps = 60000000000000, save_steps = 60000000000000
val data nums = 10, training_nums = 19808859, batch_size = 1024
Using auto half precision backend
***** Running training *****
  Num examples = 19808859
  Num train samples = 19808859.0
  world_size = 256
  Total train batch size (w. parallel, distributed & accumulation) = 1024
  Gradient Accumulation steps = 1
  Total optimization steps = 19345
[2024-07-12 16:00:42,671] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=25d5540, git-branch=HEAD

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 272.09it/s]

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010501 19640 ProcessGroupNCCL.cpp:835] [Rank 64] NCCL watchdog thread started!
I0712 16:00:43.010505 15411 ProcessGroupNCCL.cpp:669] [Rank 64] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010347 20855 ProcessGroupNCCL.cpp:835] [Rank 162] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010461  4845 ProcessGroupNCCL.cpp:835] [Rank 101] NCCL watchdog thread started!
I0712 16:00:43.010447   700 ProcessGroupNCCL.cpp:669] [Rank 101] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010186 10195 ProcessGroupNCCL.cpp:835] [Rank 42] NCCL watchdog thread started!
I0712 16:00:43.010178  6007 ProcessGroupNCCL.cpp:669] [Rank 42] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010345 16660 ProcessGroupNCCL.cpp:669] [Rank 162] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010936   701 ProcessGroupNCCL.cpp:669] [Rank 102] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010665 20050 ProcessGroupNCCL.cpp:835] [Rank 51] NCCL watchdog thread started!
I0712 16:00:43.010653 15843 ProcessGroupNCCL.cpp:669] [Rank 51] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010380 10462 ProcessGroupNCCL.cpp:835] [Rank 56] NCCL watchdog thread started!
I0712 16:00:43.010339  6244 ProcessGroupNCCL.cpp:669] [Rank 56] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010537  6167 ProcessGroupNCCL.cpp:835] [Rank 5] NCCL watchdog thread started!
I0712 16:00:43.010540  1897 ProcessGroupNCCL.cpp:669] [Rank 5] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010883 13683 ProcessGroupNCCL.cpp:835] [Rank 184] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010716  3750 ProcessGroupNCCL.cpp:835] [Rank 177] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010468  6729 ProcessGroupNCCL.cpp:835] [Rank 133] NCCL watchdog thread started!
I0712 16:00:43.010987  4846 ProcessGroupNCCL.cpp:835] [Rank 102] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010691 20051 ProcessGroupNCCL.cpp:835] [Rank 48] NCCL watchdog thread started!
I0712 16:00:43.010681 15840 ProcessGroupNCCL.cpp:669] [Rank 48] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010599  2270 ProcessGroupNCCL.cpp:835] [Rank 27] NCCL watchdog thread started!
I0712 16:00:43.010596 30541 ProcessGroupNCCL.cpp:669] [Rank 27] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010368 10461 ProcessGroupNCCL.cpp:835] [Rank 57] NCCL watchdog thread started!
I0712 16:00:43.010353  6245 ProcessGroupNCCL.cpp:669] [Rank 57] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010569  6168 ProcessGroupNCCL.cpp:835] [Rank 4] NCCL watchdog thread started!
I0712 16:00:43.010563  1896 ProcessGroupNCCL.cpp:669] [Rank 4] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010869  9354 ProcessGroupNCCL.cpp:669] [Rank 184] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010753 15812 ProcessGroupNCCL.cpp:669] [Rank 217] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010792 12804 ProcessGroupNCCL.cpp:835] [Rank 195] NCCL watchdog thread started!
I0712 16:00:43.010716 31991 ProcessGroupNCCL.cpp:669] [Rank 177] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010462  2525 ProcessGroupNCCL.cpp:669] [Rank 133] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010713 20052 ProcessGroupNCCL.cpp:835] [Rank 50] NCCL watchdog thread started!
I0712 16:00:43.010710 15842 ProcessGroupNCCL.cpp:669] [Rank 50] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010659 23586 ProcessGroupNCCL.cpp:835] [Rank 12] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010901 13684 ProcessGroupNCCL.cpp:835] [Rank 185] NCCL watchdog thread started!
I0712 16:00:43.010766 20096 ProcessGroupNCCL.cpp:835] [Rank 217] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010813 12805 ProcessGroupNCCL.cpp:835] [Rank 193] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010759  3751 ProcessGroupNCCL.cpp:835] [Rank 179] NCCL watchdog thread started!
I0712 16:00:43.010685 19185 ProcessGroupNCCL.cpp:669] [Rank 12] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010900  9355 ProcessGroupNCCL.cpp:669] [Rank 185] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010797 20097 ProcessGroupNCCL.cpp:835] [Rank 216] NCCL watchdog thread started!
I0712 16:00:43.010777  8268 ProcessGroupNCCL.cpp:669] [Rank 195] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010756 31993 ProcessGroupNCCL.cpp:669] [Rank 179] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011137  1973 ProcessGroupNCCL.cpp:835] [Rank 71] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011143  2319 ProcessGroupNCCL.cpp:835] [Rank 108] NCCL watchdog thread started!
I0712 16:00:43.011147 30225 ProcessGroupNCCL.cpp:669] [Rank 108] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010694 23587 ProcessGroupNCCL.cpp:835] [Rank 14] NCCL watchdog thread started!
I0712 16:00:43.010783 15811 ProcessGroupNCCL.cpp:669] [Rank 216] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010860 12806 ProcessGroupNCCL.cpp:835] [Rank 192] NCCL watchdog thread started!
I0712 16:00:43.010835  8265 ProcessGroupNCCL.cpp:669] [Rank 192] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011124 30015 ProcessGroupNCCL.cpp:669] [Rank 71] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011624 18064 ProcessGroupNCCL.cpp:835] [Rank 114] NCCL watchdog thread started!
I0712 16:00:43.011626 11102 ProcessGroupNCCL.cpp:669] [Rank 114] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011198  6965 ProcessGroupNCCL.cpp:835] [Rank 72] NCCL watchdog thread started!
I0712 16:00:43.011185  2708 ProcessGroupNCCL.cpp:669] [Rank 72] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010689 19187 ProcessGroupNCCL.cpp:669] [Rank 14] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011206 30514 ProcessGroupNCCL.cpp:835] [Rank 173] NCCL watchdog thread started!
I0712 16:00:43.011209 26109 ProcessGroupNCCL.cpp:669] [Rank 173] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010811 15813 ProcessGroupNCCL.cpp:669] [Rank 218] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010807  8266 ProcessGroupNCCL.cpp:669] [Rank 193] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011181 14663 ProcessGroupNCCL.cpp:835] [Rank 80] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011062  1145 ProcessGroupNCCL.cpp:669] [Rank 208] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011358 17946 ProcessGroupNCCL.cpp:835] [Rank 158] NCCL watchdog thread started!
I0712 16:00:43.011351 13749 ProcessGroupNCCL.cpp:669] [Rank 158] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011389 21416 ProcessGroupNCCL.cpp:835] [Rank 3] NCCL watchdog thread started!
I0712 16:00:43.011404 17134 ProcessGroupNCCL.cpp:669] [Rank 3] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011670 18065 ProcessGroupNCCL.cpp:835] [Rank 112] NCCL watchdog thread started!
I0712 16:00:43.011662 11100 ProcessGroupNCCL.cpp:669] [Rank 112] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011222  6966 ProcessGroupNCCL.cpp:835] [Rank 73] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011366 16344 ProcessGroupNCCL.cpp:835] [Rank 22] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.010763 23588 ProcessGroupNCCL.cpp:835] [Rank 15] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011238 30515 ProcessGroupNCCL.cpp:835] [Rank 175] NCCL watchdog thread started!
I0712 16:00:43.011232 26111 ProcessGroupNCCL.cpp:669] [Rank 175] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010823 20098 ProcessGroupNCCL.cpp:835] [Rank 218] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011173 14662 ProcessGroupNCCL.cpp:835] [Rank 83] NCCL watchdog thread started!
I0712 16:00:43.011142 10350 ProcessGroupNCCL.cpp:669] [Rank 83] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011130  5293 ProcessGroupNCCL.cpp:835] [Rank 208] NCCL watchdog thread started!
I0712 16:00:43.011216  2709 ProcessGroupNCCL.cpp:669] [Rank 73] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.010737 19188 ProcessGroupNCCL.cpp:669] [Rank 15] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011157 10347 ProcessGroupNCCL.cpp:669] [Rank 80] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011178  5292 ProcessGroupNCCL.cpp:835] [Rank 211] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011402 17133 ProcessGroupNCCL.cpp:669] [Rank 2] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011888 20328 ProcessGroupNCCL.cpp:835] [Rank 130] NCCL watchdog thread started!
I0712 16:00:43.011891 16070 ProcessGroupNCCL.cpp:669] [Rank 130] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011328  3523 ProcessGroupNCCL.cpp:835] [Rank 146] NCCL watchdog thread started!
I0712 16:00:43.011343 11829 ProcessGroupNCCL.cpp:669] [Rank 22] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011703  3957 ProcessGroupNCCL.cpp:835] [Rank 117] NCCL watchdog thread started!
I0712 16:00:43.011710 32204 ProcessGroupNCCL.cpp:669] [Rank 117] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011480 11045 ProcessGroupNCCL.cpp:669] [Rank 44] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011588 15396 ProcessGroupNCCL.cpp:835] [Rank 44] NCCL watchdog thread started!
I0712 16:00:43.011193  1148 ProcessGroupNCCL.cpp:669] [Rank 211] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011432 21415 ProcessGroupNCCL.cpp:835] [Rank 2] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011564 11821 ProcessGroupNCCL.cpp:835] [Rank 153] NCCL watchdog thread started!
I0712 16:00:43.011564  7559 ProcessGroupNCCL.cpp:669] [Rank 153] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011375  3524 ProcessGroupNCCL.cpp:835] [Rank 144] NCCL watchdog thread started!
I0712 16:00:43.011353 31777 ProcessGroupNCCL.cpp:669] [Rank 144] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011401 16345 ProcessGroupNCCL.cpp:835] [Rank 23] NCCL watchdog thread started!
I0712 16:00:43.011387 11830 ProcessGroupNCCL.cpp:669] [Rank 23] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011229  5294 ProcessGroupNCCL.cpp:835] [Rank 209] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011893 17947 ProcessGroupNCCL.cpp:835] [Rank 157] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011454 17131 ProcessGroupNCCL.cpp:669] [Rank 0] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011943 20329 ProcessGroupNCCL.cpp:835] [Rank 131] NCCL watchdog thread started!
I0712 16:00:43.011927 16071 ProcessGroupNCCL.cpp:669] [Rank 131] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011335 31779 ProcessGroupNCCL.cpp:669] [Rank 146] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011432 16346 ProcessGroupNCCL.cpp:835] [Rank 20] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011741 13396 ProcessGroupNCCL.cpp:835] [Rank 188] NCCL watchdog thread started!
I0712 16:00:43.011744  8867 ProcessGroupNCCL.cpp:669] [Rank 188] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011586  8283 ProcessGroupNCCL.cpp:835] [Rank 234] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011509 27465 ProcessGroupNCCL.cpp:835] [Rank 241] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011969 20856 ProcessGroupNCCL.cpp:835] [Rank 160] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011466 15398 ProcessGroupNCCL.cpp:835] [Rank 46] NCCL watchdog thread started!
I0712 16:00:43.011451 11047 ProcessGroupNCCL.cpp:669] [Rank 46] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011659   450 ProcessGroupNCCL.cpp:835] [Rank 85] NCCL watchdog thread started!
I0712 16:00:43.011215  1146 ProcessGroupNCCL.cpp:669] [Rank 209] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011890 13748 ProcessGroupNCCL.cpp:669] [Rank 157] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011471 21417 ProcessGroupNCCL.cpp:835] [Rank 0] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011474 24234 ProcessGroupNCCL.cpp:835] [Rank 105] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011974 20330 ProcessGroupNCCL.cpp:835] [Rank 129] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011698 11824 ProcessGroupNCCL.cpp:835] [Rank 152] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011406  3525 ProcessGroupNCCL.cpp:835] [Rank 145] NCCL watchdog thread started!
I0712 16:00:43.011422 11827 ProcessGroupNCCL.cpp:669] [Rank 20] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011780 13395 ProcessGroupNCCL.cpp:835] [Rank 191] NCCL watchdog thread started!
I0712 16:00:43.011770  8870 ProcessGroupNCCL.cpp:669] [Rank 191] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011771 26719 ProcessGroupNCCL.cpp:835] [Rank 198] NCCL watchdog thread started!
I0712 16:00:43.011560  3967 ProcessGroupNCCL.cpp:669] [Rank 234] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011559 23247 ProcessGroupNCCL.cpp:669] [Rank 241] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011933 16658 ProcessGroupNCCL.cpp:669] [Rank 160] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011631 15397 ProcessGroupNCCL.cpp:835] [Rank 45] NCCL watchdog thread started!
I0712 16:00:43.011644 28778 ProcessGroupNCCL.cpp:669] [Rank 85] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011471 19919 ProcessGroupNCCL.cpp:669] [Rank 105] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012007 20331 ProcessGroupNCCL.cpp:835] [Rank 128] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011670 11822 ProcessGroupNCCL.cpp:835] [Rank 155] NCCL watchdog thread started!
I0712 16:00:43.011394 31778 ProcessGroupNCCL.cpp:669] [Rank 145] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011770 22575 ProcessGroupNCCL.cpp:669] [Rank 198] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011585  8284 ProcessGroupNCCL.cpp:835] [Rank 235] NCCL watchdog thread started!
I0712 16:00:43.011574  3968 ProcessGroupNCCL.cpp:669] [Rank 235] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011812 27466 ProcessGroupNCCL.cpp:835] [Rank 242] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011838  6730 ProcessGroupNCCL.cpp:835] [Rank 134] NCCL watchdog thread started!
I0712 16:00:43.011620 11046 ProcessGroupNCCL.cpp:669] [Rank 45] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011682   451 ProcessGroupNCCL.cpp:835] [Rank 87] NCCL watchdog thread started!
I0712 16:00:43.011672 28780 ProcessGroupNCCL.cpp:669] [Rank 87] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012102 14928 ProcessGroupNCCL.cpp:835] [Rank 9] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011507 24235 ProcessGroupNCCL.cpp:835] [Rank 107] NCCL watchdog thread started!
I0712 16:00:43.011508 19921 ProcessGroupNCCL.cpp:669] [Rank 107] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011963 16069 ProcessGroupNCCL.cpp:669] [Rank 129] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011691  7558 ProcessGroupNCCL.cpp:669] [Rank 152] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012146 31935 ProcessGroupNCCL.cpp:835] [Rank 35] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011545  3526 ProcessGroupNCCL.cpp:835] [Rank 147] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012125  2710 ProcessGroupNCCL.cpp:669] [Rank 74] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012409  9973 ProcessGroupNCCL.cpp:835] [Rank 127] NCCL watchdog thread started!
I0712 16:00:43.012415  5406 ProcessGroupNCCL.cpp:669] [Rank 127] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011806 23248 ProcessGroupNCCL.cpp:669] [Rank 242] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011821  2526 ProcessGroupNCCL.cpp:669] [Rank 134] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012074 10642 ProcessGroupNCCL.cpp:669] [Rank 9] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011997 16068 ProcessGroupNCCL.cpp:669] [Rank 128] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011660  7561 ProcessGroupNCCL.cpp:669] [Rank 155] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012176 31936 ProcessGroupNCCL.cpp:835] [Rank 33] NCCL watchdog thread started!
I0712 16:00:43.012161 27715 ProcessGroupNCCL.cpp:669] [Rank 33] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.011543 31780 ProcessGroupNCCL.cpp:669] [Rank 147] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012311 10196 ProcessGroupNCCL.cpp:835] [Rank 41] NCCL watchdog thread started!
I0712 16:00:43.012132  6967 ProcessGroupNCCL.cpp:835] [Rank 74] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012473  9974 ProcessGroupNCCL.cpp:835] [Rank 125] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012507 13567 ProcessGroupNCCL.cpp:835] [Rank 91] NCCL watchdog thread started!
I0712 16:00:43.012507  9022 ProcessGroupNCCL.cpp:669] [Rank 91] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011865 27467 ProcessGroupNCCL.cpp:835] [Rank 240] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012117 14929 ProcessGroupNCCL.cpp:835] [Rank 11] NCCL watchdog thread started!
I0712 16:00:43.012089 10644 ProcessGroupNCCL.cpp:669] [Rank 11] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.011847 11829 ProcessGroupNCCL.cpp:835] [Rank 154] NCCL watchdog thread started!
I0712 16:00:43.012132 27717 ProcessGroupNCCL.cpp:669] [Rank 35] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012285  6006 ProcessGroupNCCL.cpp:669] [Rank 41] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012465  5404 ProcessGroupNCCL.cpp:669] [Rank 125] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012549 13568 ProcessGroupNCCL.cpp:835] [Rank 90] NCCL watchdog thread started!
I0712 16:00:43.011866 23246 ProcessGroupNCCL.cpp:669] [Rank 240] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012239 26885 ProcessGroupNCCL.cpp:835] [Rank 122] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012148 14930 ProcessGroupNCCL.cpp:835] [Rank 8] NCCL watchdog thread started!
I0712 16:00:43.011802  7560 ProcessGroupNCCL.cpp:669] [Rank 154] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012190 27714 ProcessGroupNCCL.cpp:669] [Rank 32] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012892  5403 ProcessGroupNCCL.cpp:669] [Rank 124] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012539  9021 ProcessGroupNCCL.cpp:669] [Rank 90] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012220 22662 ProcessGroupNCCL.cpp:669] [Rank 122] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012773 30013 ProcessGroupNCCL.cpp:669] [Rank 69] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012136 10641 ProcessGroupNCCL.cpp:669] [Rank 8] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012197 31937 ProcessGroupNCCL.cpp:835] [Rank 32] NCCL watchdog thread started!
I0712 16:00:43.012948  9975 ProcessGroupNCCL.cpp:835] [Rank 124] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012341 26886 ProcessGroupNCCL.cpp:835] [Rank 120] NCCL watchdog thread started!
I0712 16:00:43.012818  1974 ProcessGroupNCCL.cpp:835] [Rank 69] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012161 14931 ProcessGroupNCCL.cpp:835] [Rank 10] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012218 27716 ProcessGroupNCCL.cpp:669] [Rank 34] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012331 22660 ProcessGroupNCCL.cpp:669] [Rank 120] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012161 10643 ProcessGroupNCCL.cpp:669] [Rank 10] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012228 31938 ProcessGroupNCCL.cpp:835] [Rank 34] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012794 10197 ProcessGroupNCCL.cpp:835] [Rank 40] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012652  5697 ProcessGroupNCCL.cpp:835] [Rank 30] NCCL watchdog thread started!
I0712 16:00:43.012665  1521 ProcessGroupNCCL.cpp:669] [Rank 30] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012835  6929 ProcessGroupNCCL.cpp:835] [Rank 52] NCCL watchdog thread started!
I0712 16:00:43.012862  2755 ProcessGroupNCCL.cpp:669] [Rank 52] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012897 14664 ProcessGroupNCCL.cpp:835] [Rank 82] NCCL watchdog thread started!
I0712 16:00:43.012763  6005 ProcessGroupNCCL.cpp:669] [Rank 40] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012679  5698 ProcessGroupNCCL.cpp:835] [Rank 31] NCCL watchdog thread started!
I0712 16:00:43.012671  1522 ProcessGroupNCCL.cpp:669] [Rank 31] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012982  6930 ProcessGroupNCCL.cpp:835] [Rank 54] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013036 30323 ProcessGroupNCCL.cpp:835] [Rank 165] NCCL watchdog thread started!
I0712 16:00:43.013036 26251 ProcessGroupNCCL.cpp:669] [Rank 165] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012859 10349 ProcessGroupNCCL.cpp:669] [Rank 82] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.012717  5699 ProcessGroupNCCL.cpp:835] [Rank 29] NCCL watchdog thread started!
I0712 16:00:43.012969  2757 ProcessGroupNCCL.cpp:669] [Rank 54] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013307 13685 ProcessGroupNCCL.cpp:835] [Rank 187] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013067 14712 ProcessGroupNCCL.cpp:835] [Rank 238] NCCL watchdog thread started!
I0712 16:00:43.013072 10386 ProcessGroupNCCL.cpp:669] [Rank 238] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.012707  1520 ProcessGroupNCCL.cpp:669] [Rank 29] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.013278  9357 ProcessGroupNCCL.cpp:669] [Rank 187] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013430  4847 ProcessGroupNCCL.cpp:835] [Rank 103] NCCL watchdog thread started!
I0712 16:00:43.013415   702 ProcessGroupNCCL.cpp:669] [Rank 103] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013554 19641 ProcessGroupNCCL.cpp:835] [Rank 65] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013717  9976 ProcessGroupNCCL.cpp:835] [Rank 126] NCCL watchdog thread started!
I0712 16:00:43.013512 15412 ProcessGroupNCCL.cpp:669] [Rank 65] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.013684  5405 ProcessGroupNCCL.cpp:669] [Rank 126] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013819 19642 ProcessGroupNCCL.cpp:835] [Rank 67] NCCL watchdog thread started!
I0712 16:00:43.013779 15414 ProcessGroupNCCL.cpp:669] [Rank 67] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013545  4690 ProcessGroupNCCL.cpp:835] [Rank 169] NCCL watchdog thread started!
I0712 16:00:43.013553   476 ProcessGroupNCCL.cpp:669] [Rank 169] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013587 12807 ProcessGroupNCCL.cpp:835] [Rank 194] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013636 17955 ProcessGroupNCCL.cpp:835] [Rank 62] NCCL watchdog thread started!
I0712 16:00:43.013584  8267 ProcessGroupNCCL.cpp:669] [Rank 194] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013221 32128 ProcessGroupNCCL.cpp:835] [Rank 97] NCCL watchdog thread started!
I0712 16:00:43.013634 13723 ProcessGroupNCCL.cpp:669] [Rank 62] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013602  4691 ProcessGroupNCCL.cpp:835] [Rank 170] NCCL watchdog thread started!
I0712 16:00:43.013597   477 ProcessGroupNCCL.cpp:669] [Rank 170] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013797  3752 ProcessGroupNCCL.cpp:835] [Rank 176] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013245  7620 ProcessGroupNCCL.cpp:835] [Rank 253] NCCL watchdog thread started!
I0712 16:00:43.013237  3309 ProcessGroupNCCL.cpp:669] [Rank 253] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.013263 32129 ProcessGroupNCCL.cpp:835] [Rank 96] NCCL watchdog thread started!
I0712 16:00:43.013775 31990 ProcessGroupNCCL.cpp:669] [Rank 176] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.013208 27926 ProcessGroupNCCL.cpp:669] [Rank 97] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014119 15413 ProcessGroupNCCL.cpp:669] [Rank 66] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.013252 27925 ProcessGroupNCCL.cpp:669] [Rank 96] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.014133 19643 ProcessGroupNCCL.cpp:835] [Rank 66] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014005 24236 ProcessGroupNCCL.cpp:835] [Rank 104] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014372 14713 ProcessGroupNCCL.cpp:835] [Rank 236] NCCL watchdog thread started!
I0712 16:00:43.013999 19918 ProcessGroupNCCL.cpp:669] [Rank 104] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.014334 10384 ProcessGroupNCCL.cpp:669] [Rank 236] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014254 24237 ProcessGroupNCCL.cpp:835] [Rank 106] NCCL watchdog thread started!
I0712 16:00:43.014248 19920 ProcessGroupNCCL.cpp:669] [Rank 106] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014528 26720 ProcessGroupNCCL.cpp:835] [Rank 197] NCCL watchdog thread started!
I0712 16:00:43.014497 22574 ProcessGroupNCCL.cpp:669] [Rank 197] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014863  2320 ProcessGroupNCCL.cpp:835] [Rank 111] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014922 21603 ProcessGroupNCCL.cpp:835] [Rank 95] NCCL watchdog thread started!
I0712 16:00:43.014909 17405 ProcessGroupNCCL.cpp:669] [Rank 95] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.014833 30228 ProcessGroupNCCL.cpp:669] [Rank 111] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014948 21604 ProcessGroupNCCL.cpp:835] [Rank 94] NCCL watchdog thread started!
I0712 16:00:43.014940 17404 ProcessGroupNCCL.cpp:669] [Rank 94] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.014909  5434 ProcessGroupNCCL.cpp:835] [Rank 251] NCCL watchdog thread started!
I0712 16:00:43.014909  1169 ProcessGroupNCCL.cpp:669] [Rank 251] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.015271 15399 ProcessGroupNCCL.cpp:835] [Rank 47] NCCL watchdog thread started!
I0712 16:00:43.015267 11048 ProcessGroupNCCL.cpp:669] [Rank 47] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.015681  2321 ProcessGroupNCCL.cpp:835] [Rank 109] NCCL watchdog thread started!
I0712 16:00:43.015664 30226 ProcessGroupNCCL.cpp:669] [Rank 109] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.015476 26887 ProcessGroupNCCL.cpp:835] [Rank 123] NCCL watchdog thread started!
I0712 16:00:43.015466 22663 ProcessGroupNCCL.cpp:669] [Rank 123] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.016486  5700 ProcessGroupNCCL.cpp:835] [Rank 28] NCCL watchdog thread started!
I0712 16:00:43.016484  1519 ProcessGroupNCCL.cpp:669] [Rank 28] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.016928  6931 ProcessGroupNCCL.cpp:835] [Rank 55] NCCL watchdog thread started!
I0712 16:00:43.016912  2758 ProcessGroupNCCL.cpp:669] [Rank 55] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.017128  6933 ProcessGroupNCCL.cpp:835] [Rank 214] NCCL watchdog thread started!
I0712 16:00:43.017128  2602 ProcessGroupNCCL.cpp:669] [Rank 214] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.017369 30014 ProcessGroupNCCL.cpp:669] [Rank 70] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.017410  1975 ProcessGroupNCCL.cpp:835] [Rank 70] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.017138 29990 ProcessGroupNCCL.cpp:669] [Rank 225] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.017154  1627 ProcessGroupNCCL.cpp:835] [Rank 225] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.017189  1628 ProcessGroupNCCL.cpp:835] [Rank 224] NCCL watchdog thread started!
I0712 16:00:43.017181 29989 ProcessGroupNCCL.cpp:669] [Rank 224] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.018041 14714 ProcessGroupNCCL.cpp:835] [Rank 239] NCCL watchdog thread started!
I0712 16:00:43.018008 10387 ProcessGroupNCCL.cpp:669] [Rank 239] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.018247  4692 ProcessGroupNCCL.cpp:835] [Rank 171] NCCL watchdog thread started!
I0712 16:00:43.018247   478 ProcessGroupNCCL.cpp:669] [Rank 171] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.018358 32130 ProcessGroupNCCL.cpp:835] [Rank 99] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.018844 31992 ProcessGroupNCCL.cpp:669] [Rank 178] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.018332 27928 ProcessGroupNCCL.cpp:669] [Rank 99] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.018890  3753 ProcessGroupNCCL.cpp:835] [Rank 178] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.018838  1168 ProcessGroupNCCL.cpp:669] [Rank 250] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.018890  5435 ProcessGroupNCCL.cpp:835] [Rank 250] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019215 17062 ProcessGroupNCCL.cpp:835] [Rank 138] NCCL watchdog thread started!
I0712 16:00:43.019225 12849 ProcessGroupNCCL.cpp:669] [Rank 138] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019567 21454 ProcessGroupNCCL.cpp:835] [Rank 149] NCCL watchdog thread started!
I0712 16:00:43.019572 17283 ProcessGroupNCCL.cpp:669] [Rank 149] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019388  5436 ProcessGroupNCCL.cpp:835] [Rank 248] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019712 14550 ProcessGroupNCCL.cpp:835] [Rank 203] NCCL watchdog thread started!
I0712 16:00:43.019346  1166 ProcessGroupNCCL.cpp:669] [Rank 248] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019467  1629 ProcessGroupNCCL.cpp:835] [Rank 226] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019739 14551 ProcessGroupNCCL.cpp:835] [Rank 202] NCCL watchdog thread started!
I0712 16:00:43.019465 29991 ProcessGroupNCCL.cpp:669] [Rank 226] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.019698 10223 ProcessGroupNCCL.cpp:669] [Rank 203] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.019728 10222 ProcessGroupNCCL.cpp:669] [Rank 202] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019547  7621 ProcessGroupNCCL.cpp:835] [Rank 252] NCCL watchdog thread started!
I0712 16:00:43.019497  3308 ProcessGroupNCCL.cpp:669] [Rank 252] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.019850  1630 ProcessGroupNCCL.cpp:835] [Rank 227] NCCL watchdog thread started!
I0712 16:00:43.019845 29992 ProcessGroupNCCL.cpp:669] [Rank 227] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.020771 31192 ProcessGroupNCCL.cpp:835] [Rank 36] NCCL watchdog thread started!
I0712 16:00:43.020774 27035 ProcessGroupNCCL.cpp:669] [Rank 36] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.020255  3310 ProcessGroupNCCL.cpp:669] [Rank 254] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.020318  7622 ProcessGroupNCCL.cpp:835] [Rank 254] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.020692 29837 ProcessGroupNCCL.cpp:835] [Rank 222] NCCL watchdog thread started!
I0712 16:00:43.020681 25619 ProcessGroupNCCL.cpp:669] [Rank 222] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.021322  9019 ProcessGroupNCCL.cpp:669] [Rank 88] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.021369 13569 ProcessGroupNCCL.cpp:835] [Rank 88] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.021319 12523 ProcessGroupNCCL.cpp:835] [Rank 206] NCCL watchdog thread started!
I0712 16:00:43.021309  8286 ProcessGroupNCCL.cpp:669] [Rank 206] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.021329 12524 ProcessGroupNCCL.cpp:835] [Rank 204] NCCL watchdog thread started!
I0712 16:00:43.021328  8284 ProcessGroupNCCL.cpp:669] [Rank 204] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.022334 32506 ProcessGroupNCCL.cpp:835] [Rank 76] NCCL watchdog thread started!
I0712 16:00:43.022336 28353 ProcessGroupNCCL.cpp:669] [Rank 76] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.022367 32507 ProcessGroupNCCL.cpp:835] [Rank 77] NCCL watchdog thread started!
I0712 16:00:43.022356 28354 ProcessGroupNCCL.cpp:669] [Rank 77] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.023048 17063 ProcessGroupNCCL.cpp:835] [Rank 139] NCCL watchdog thread started!
I0712 16:00:43.023041 12850 ProcessGroupNCCL.cpp:669] [Rank 139] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.024268  2600 ProcessGroupNCCL.cpp:669] [Rank 212] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.024317  6934 ProcessGroupNCCL.cpp:835] [Rank 212] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.024497  3958 ProcessGroupNCCL.cpp:835] [Rank 116] NCCL watchdog thread started!
I0712 16:00:43.024463 32203 ProcessGroupNCCL.cpp:669] [Rank 116] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.024639 21455 ProcessGroupNCCL.cpp:835] [Rank 148] NCCL watchdog thread started!
I0712 16:00:43.024633 17282 ProcessGroupNCCL.cpp:669] [Rank 148] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.024673 21456 ProcessGroupNCCL.cpp:835] [Rank 150] NCCL watchdog thread started!
I0712 16:00:43.024673 17284 ProcessGroupNCCL.cpp:669] [Rank 150] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025053  2603 ProcessGroupNCCL.cpp:669] [Rank 215] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.025107  6935 ProcessGroupNCCL.cpp:835] [Rank 215] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025441 31193 ProcessGroupNCCL.cpp:835] [Rank 38] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.024945  7623 ProcessGroupNCCL.cpp:835] [Rank 255] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025300 28356 ProcessGroupNCCL.cpp:669] [Rank 79] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.025415 27037 ProcessGroupNCCL.cpp:669] [Rank 38] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025175 14494 ProcessGroupNCCL.cpp:835] [Rank 180] NCCL watchdog thread started!
I0712 16:00:43.025164 10155 ProcessGroupNCCL.cpp:669] [Rank 180] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.024911  3311 ProcessGroupNCCL.cpp:669] [Rank 255] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.025342 32508 ProcessGroupNCCL.cpp:835] [Rank 79] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025620 31194 ProcessGroupNCCL.cpp:835] [Rank 37] NCCL watchdog thread started!
I0712 16:00:43.025586 27036 ProcessGroupNCCL.cpp:669] [Rank 37] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025909  2322 ProcessGroupNCCL.cpp:835] [Rank 110] NCCL watchdog thread started!
I0712 16:00:43.025879 30227 ProcessGroupNCCL.cpp:669] [Rank 110] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.026010 12525 ProcessGroupNCCL.cpp:835] [Rank 205] NCCL watchdog thread started!
I0712 16:00:43.025972  8285 ProcessGroupNCCL.cpp:669] [Rank 205] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.026948 22478 ProcessGroupNCCL.cpp:835] [Rank 143] NCCL watchdog thread started!
I0712 16:00:43.026907 18264 ProcessGroupNCCL.cpp:669] [Rank 143] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.026934 22477 ProcessGroupNCCL.cpp:835] [Rank 141] NCCL watchdog thread started!
I0712 16:00:43.026921 18262 ProcessGroupNCCL.cpp:669] [Rank 141] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.026966 22479 ProcessGroupNCCL.cpp:835] [Rank 142] NCCL watchdog thread started!
I0712 16:00:43.026966 18263 ProcessGroupNCCL.cpp:669] [Rank 142] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.026813 14495 ProcessGroupNCCL.cpp:835] [Rank 181] NCCL watchdog thread started!
I0712 16:00:43.026788 10156 ProcessGroupNCCL.cpp:669] [Rank 181] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.027470 14496 ProcessGroupNCCL.cpp:835] [Rank 182] NCCL watchdog thread started!
I0712 16:00:43.027449 10157 ProcessGroupNCCL.cpp:669] [Rank 182] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.025836 10916 ProcessGroupNCCL.cpp:835] [Rank 16] NCCL watchdog thread started!
I0712 16:00:43.025844  6667 ProcessGroupNCCL.cpp:669] [Rank 16] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.026114 10917 ProcessGroupNCCL.cpp:835] [Rank 17] NCCL watchdog thread started!
I0712 16:00:43.026070  6668 ProcessGroupNCCL.cpp:669] [Rank 17] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.028649 29838 ProcessGroupNCCL.cpp:835] [Rank 223] NCCL watchdog thread started!
I0712 16:00:43.028614 25620 ProcessGroupNCCL.cpp:669] [Rank 223] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.029417 21605 ProcessGroupNCCL.cpp:835] [Rank 93] NCCL watchdog thread started!
I0712 16:00:43.029393 17403 ProcessGroupNCCL.cpp:669] [Rank 93] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.029222 17064 ProcessGroupNCCL.cpp:835] [Rank 137] NCCL watchdog thread started!
I0712 16:00:43.029196 12848 ProcessGroupNCCL.cpp:669] [Rank 137] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.029408 29839 ProcessGroupNCCL.cpp:835] [Rank 220] NCCL watchdog thread started!
I0712 16:00:43.029363 25617 ProcessGroupNCCL.cpp:669] [Rank 220] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.030754 11101 ProcessGroupNCCL.cpp:669] [Rank 113] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.030563 21457 ProcessGroupNCCL.cpp:835] [Rank 151] NCCL watchdog thread started!
I0712 16:00:43.030803 18066 ProcessGroupNCCL.cpp:835] [Rank 113] NCCL watchdog thread started!
I0712 16:00:43.030537 17285 ProcessGroupNCCL.cpp:669] [Rank 151] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.030635 10918 ProcessGroupNCCL.cpp:835] [Rank 18] NCCL watchdog thread started!
I0712 16:00:43.030586  6669 ProcessGroupNCCL.cpp:669] [Rank 18] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.033569 29840 ProcessGroupNCCL.cpp:835] [Rank 221] NCCL watchdog thread started!
I0712 16:00:43.033543 25618 ProcessGroupNCCL.cpp:669] [Rank 221] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.034276 10385 ProcessGroupNCCL.cpp:669] [Rank 237] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.034325 14715 ProcessGroupNCCL.cpp:835] [Rank 237] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.037367  3008 ProcessGroupNCCL.cpp:835] [Rank 230] NCCL watchdog thread started!
I0712 16:00:43.037366 31264 ProcessGroupNCCL.cpp:669] [Rank 230] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.038409  6731 ProcessGroupNCCL.cpp:835] [Rank 132] NCCL watchdog thread started!
I0712 16:00:43.038364  2524 ProcessGroupNCCL.cpp:669] [Rank 132] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.039971  3009 ProcessGroupNCCL.cpp:835] [Rank 228] NCCL watchdog thread started!
I0712 16:00:43.039937 31262 ProcessGroupNCCL.cpp:669] [Rank 228] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.041147   699 ProcessGroupNCCL.cpp:669] [Rank 100] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.041193  4848 ProcessGroupNCCL.cpp:835] [Rank 100] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.041829  5764 ProcessGroupNCCL.cpp:835] [Rank 247] NCCL watchdog thread started!
I0712 16:00:43.041823  1507 ProcessGroupNCCL.cpp:669] [Rank 247] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.042037 26721 ProcessGroupNCCL.cpp:835] [Rank 199] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.042012  5765 ProcessGroupNCCL.cpp:835] [Rank 246] NCCL watchdog thread started!
I0712 16:00:43.041977  1506 ProcessGroupNCCL.cpp:669] [Rank 246] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.041987 22576 ProcessGroupNCCL.cpp:669] [Rank 199] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.043229 17956 ProcessGroupNCCL.cpp:835] [Rank 60] NCCL watchdog thread started!
I0712 16:00:43.043190 13721 ProcessGroupNCCL.cpp:669] [Rank 60] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.044914 27038 ProcessGroupNCCL.cpp:669] [Rank 39] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.044961 31195 ProcessGroupNCCL.cpp:835] [Rank 39] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.046660 13570 ProcessGroupNCCL.cpp:835] [Rank 89] NCCL watchdog thread started!
I0712 16:00:43.046635  9020 ProcessGroupNCCL.cpp:669] [Rank 89] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.049214 13405 ProcessGroupNCCL.cpp:835] [Rank 190] NCCL watchdog thread started!
I0712 16:00:43.049178  8869 ProcessGroupNCCL.cpp:669] [Rank 190] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.053933 30324 ProcessGroupNCCL.cpp:835] [Rank 167] NCCL watchdog thread started!
I0712 16:00:43.053915 26253 ProcessGroupNCCL.cpp:669] [Rank 167] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.054443 30516 ProcessGroupNCCL.cpp:835] [Rank 172] NCCL watchdog thread started!
I0712 16:00:43.054401 26108 ProcessGroupNCCL.cpp:669] [Rank 172] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.058574  1147 ProcessGroupNCCL.cpp:669] [Rank 210] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.058620  5295 ProcessGroupNCCL.cpp:835] [Rank 210] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.058769  6169 ProcessGroupNCCL.cpp:835] [Rank 7] NCCL watchdog thread started!
I0712 16:00:43.058743  1899 ProcessGroupNCCL.cpp:669] [Rank 7] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.058821  2271 ProcessGroupNCCL.cpp:835] [Rank 25] NCCL watchdog thread started!
I0712 16:00:43.058776 30539 ProcessGroupNCCL.cpp:669] [Rank 25] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.059057  2272 ProcessGroupNCCL.cpp:835] [Rank 24] NCCL watchdog thread started!
I0712 16:00:43.059036 30538 ProcessGroupNCCL.cpp:669] [Rank 24] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.060057  3959 ProcessGroupNCCL.cpp:835] [Rank 119] NCCL watchdog thread started!
I0712 16:00:43.060027 32206 ProcessGroupNCCL.cpp:669] [Rank 119] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.060724 10220 ProcessGroupNCCL.cpp:669] [Rank 200] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.060775 14552 ProcessGroupNCCL.cpp:835] [Rank 200] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.065121 26252 ProcessGroupNCCL.cpp:669] [Rank 166] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.065171 30325 ProcessGroupNCCL.cpp:835] [Rank 166] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.069526 12526 ProcessGroupNCCL.cpp:835] [Rank 207] NCCL watchdog thread started!
I0712 16:00:43.069504  8287 ProcessGroupNCCL.cpp:669] [Rank 207] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.070578 20100 ProcessGroupNCCL.cpp:835] [Rank 219] NCCL watchdog thread started!
I0712 16:00:43.070554 15814 ProcessGroupNCCL.cpp:669] [Rank 219] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.072109  8285 ProcessGroupNCCL.cpp:835] [Rank 233] NCCL watchdog thread started!
I0712 16:00:43.072074  3966 ProcessGroupNCCL.cpp:669] [Rank 233] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.074641  6008 ProcessGroupNCCL.cpp:669] [Rank 43] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.074680 10198 ProcessGroupNCCL.cpp:835] [Rank 43] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.075129 16347 ProcessGroupNCCL.cpp:835] [Rank 21] NCCL watchdog thread started!
I0712 16:00:43.075114 11828 ProcessGroupNCCL.cpp:669] [Rank 21] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.076171  6247 ProcessGroupNCCL.cpp:669] [Rank 59] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.076234 10463 ProcessGroupNCCL.cpp:835] [Rank 59] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.078874 17957 ProcessGroupNCCL.cpp:835] [Rank 61] NCCL watchdog thread started!
I0712 16:00:43.078842 13722 ProcessGroupNCCL.cpp:669] [Rank 61] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.081173 27927 ProcessGroupNCCL.cpp:669] [Rank 98] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.081218 32131 ProcessGroupNCCL.cpp:835] [Rank 98] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.087643 17949 ProcessGroupNCCL.cpp:835] [Rank 159] NCCL watchdog thread started!
I0712 16:00:43.087639 13750 ProcessGroupNCCL.cpp:669] [Rank 159] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.088727 13686 ProcessGroupNCCL.cpp:835] [Rank 186] NCCL watchdog thread started!
I0712 16:00:43.088711  9356 ProcessGroupNCCL.cpp:669] [Rank 186] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.088554  2273 ProcessGroupNCCL.cpp:835] [Rank 26] NCCL watchdog thread started!
I0712 16:00:43.088510 30540 ProcessGroupNCCL.cpp:669] [Rank 26] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.089023 26888 ProcessGroupNCCL.cpp:835] [Rank 121] NCCL watchdog thread started!
I0712 16:00:43.089016 22661 ProcessGroupNCCL.cpp:669] [Rank 121] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.090158 26722 ProcessGroupNCCL.cpp:835] [Rank 196] NCCL watchdog thread started!
I0712 16:00:43.090142 22573 ProcessGroupNCCL.cpp:669] [Rank 196] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.090159 28777 ProcessGroupNCCL.cpp:669] [Rank 84] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.090204   452 ProcessGroupNCCL.cpp:835] [Rank 84] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.092823 13724 ProcessGroupNCCL.cpp:669] [Rank 63] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.092869 17958 ProcessGroupNCCL.cpp:835] [Rank 63] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.097919  3965 ProcessGroupNCCL.cpp:669] [Rank 232] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.097973  8286 ProcessGroupNCCL.cpp:835] [Rank 232] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.099022 20857 ProcessGroupNCCL.cpp:835] [Rank 163] NCCL watchdog thread started!
I0712 16:00:43.099001 16661 ProcessGroupNCCL.cpp:669] [Rank 163] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.099865 18067 ProcessGroupNCCL.cpp:835] [Rank 115] NCCL watchdog thread started!
I0712 16:00:43.099828 11103 ProcessGroupNCCL.cpp:669] [Rank 115] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.102875  6968 ProcessGroupNCCL.cpp:835] [Rank 75] NCCL watchdog thread started!
I0712 16:00:43.102841  2711 ProcessGroupNCCL.cpp:669] [Rank 75] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.102766 10464 ProcessGroupNCCL.cpp:835] [Rank 58] NCCL watchdog thread started!
I0712 16:00:43.102737  6246 ProcessGroupNCCL.cpp:669] [Rank 58] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.103736 17950 ProcessGroupNCCL.cpp:835] [Rank 156] NCCL watchdog thread started!
I0712 16:00:43.103727 13747 ProcessGroupNCCL.cpp:669] [Rank 156] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.107591  3010 ProcessGroupNCCL.cpp:835] [Rank 229] NCCL watchdog thread started!
I0712 16:00:43.107554 31263 ProcessGroupNCCL.cpp:669] [Rank 229] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.111456  6170 ProcessGroupNCCL.cpp:835] [Rank 6] NCCL watchdog thread started!
I0712 16:00:43.111423  1898 ProcessGroupNCCL.cpp:669] [Rank 6] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.115077 32205 ProcessGroupNCCL.cpp:669] [Rank 118] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.115121  3960 ProcessGroupNCCL.cpp:835] [Rank 118] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.117187 10158 ProcessGroupNCCL.cpp:669] [Rank 183] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.117237 14497 ProcessGroupNCCL.cpp:835] [Rank 183] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.124585  8868 ProcessGroupNCCL.cpp:669] [Rank 189] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.124627 13406 ProcessGroupNCCL.cpp:835] [Rank 189] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.135064 23589 ProcessGroupNCCL.cpp:835] [Rank 13] NCCL watchdog thread started!
I0712 16:00:43.135041 19186 ProcessGroupNCCL.cpp:669] [Rank 13] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.135619  6932 ProcessGroupNCCL.cpp:835] [Rank 53] NCCL watchdog thread started!
I0712 16:00:43.135607  2756 ProcessGroupNCCL.cpp:669] [Rank 53] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.138868 10919 ProcessGroupNCCL.cpp:835] [Rank 19] NCCL watchdog thread started!
I0712 16:00:43.138828  6670 ProcessGroupNCCL.cpp:669] [Rank 19] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.141577 30517 ProcessGroupNCCL.cpp:835] [Rank 174] NCCL watchdog thread started!
I0712 16:00:43.141558 26110 ProcessGroupNCCL.cpp:669] [Rank 174] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.143100 12847 ProcessGroupNCCL.cpp:669] [Rank 136] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.143121 17065 ProcessGroupNCCL.cpp:835] [Rank 136] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.145104  1504 ProcessGroupNCCL.cpp:669] [Rank 244] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.145153  5766 ProcessGroupNCCL.cpp:835] [Rank 244] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.152362 26250 ProcessGroupNCCL.cpp:669] [Rank 164] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.152379 30326 ProcessGroupNCCL.cpp:835] [Rank 164] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.156095 14553 ProcessGroupNCCL.cpp:835] [Rank 201] NCCL watchdog thread started!
I0712 16:00:43.156075 10221 ProcessGroupNCCL.cpp:669] [Rank 201] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.158041 22480 ProcessGroupNCCL.cpp:835] [Rank 140] NCCL watchdog thread started!
I0712 16:00:43.158000 18261 ProcessGroupNCCL.cpp:669] [Rank 140] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.161021 32509 ProcessGroupNCCL.cpp:835] [Rank 78] NCCL watchdog thread started!
I0712 16:00:43.160988 28355 ProcessGroupNCCL.cpp:669] [Rank 78] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.161720  6936 ProcessGroupNCCL.cpp:835] [Rank 213] NCCL watchdog thread started!
I0712 16:00:43.161680  2601 ProcessGroupNCCL.cpp:669] [Rank 213] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.170127  5767 ProcessGroupNCCL.cpp:835] [Rank 245] NCCL watchdog thread started!
I0712 16:00:43.170101  1505 ProcessGroupNCCL.cpp:669] [Rank 245] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.187800 20054 ProcessGroupNCCL.cpp:835] [Rank 49] NCCL watchdog thread started!
I0712 16:00:43.187780 15841 ProcessGroupNCCL.cpp:669] [Rank 49] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.188243 27468 ProcessGroupNCCL.cpp:835] [Rank 243] NCCL watchdog thread started!
I0712 16:00:43.188230 23249 ProcessGroupNCCL.cpp:669] [Rank 243] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.189857 20858 ProcessGroupNCCL.cpp:835] [Rank 161] NCCL watchdog thread started!
I0712 16:00:43.189819 16659 ProcessGroupNCCL.cpp:669] [Rank 161] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.200582 21418 ProcessGroupNCCL.cpp:835] [Rank 1] NCCL watchdog thread started!
I0712 16:00:43.200554 17132 ProcessGroupNCCL.cpp:669] [Rank 1] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.201073 31265 ProcessGroupNCCL.cpp:669] [Rank 231] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.201131  3017 ProcessGroupNCCL.cpp:835] [Rank 231] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.203594 14665 ProcessGroupNCCL.cpp:835] [Rank 81] NCCL watchdog thread started!
I0712 16:00:43.203567 10348 ProcessGroupNCCL.cpp:669] [Rank 81] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.204510   453 ProcessGroupNCCL.cpp:835] [Rank 86] NCCL watchdog thread started!
I0712 16:00:43.204468 28779 ProcessGroupNCCL.cpp:669] [Rank 86] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.221144  4693 ProcessGroupNCCL.cpp:835] [Rank 168] NCCL watchdog thread started!
I0712 16:00:43.221122   475 ProcessGroupNCCL.cpp:669] [Rank 168] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.223968 17402 ProcessGroupNCCL.cpp:669] [Rank 92] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
I0712 16:00:43.224017 21606 ProcessGroupNCCL.cpp:835] [Rank 92] NCCL watchdog thread started!

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.224943  6732 ProcessGroupNCCL.cpp:835] [Rank 135] NCCL watchdog thread started!
I0712 16:00:43.224905  2527 ProcessGroupNCCL.cpp:669] [Rank 135] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.225849  5437 ProcessGroupNCCL.cpp:835] [Rank 249] NCCL watchdog thread started!
I0712 16:00:43.225809  1167 ProcessGroupNCCL.cpp:669] [Rank 249] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0

Map:   0%|          | 0/10 [00:00<?, ? examples/s]
                                                  I0712 16:00:43.258905  1976 ProcessGroupNCCL.cpp:835] [Rank 68] NCCL watchdog thread started!
I0712 16:00:43.258888 30012 ProcessGroupNCCL.cpp:669] [Rank 68] ProcessGroupNCCL initialized with following options:
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_DESYNC_DEBUG: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
[2024-07-12 16:00:43,262] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-12 16:00:43,263] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-07-12 16:00:43,283] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-12 16:00:43,283] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-12 16:00:43,283] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-07-12 16:00:43,283] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer
[2024-07-12 16:00:43,360] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2024-07-12 16:00:43,361] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 1.22 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:43,362] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 31.1 GB, percent = 24.7%
[2024-07-12 16:00:43,365] [INFO] [stage3.py:113:__init__] Reduce bucket size 16777216
[2024-07-12 16:00:43,365] [INFO] [stage3.py:114:__init__] Prefetch bucket size 15099494
[2024-07-12 16:00:44,791] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-07-12 16:00:44,792] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:44,793] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 31.1 GB, percent = 24.7%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-07-12 16:00:44,894] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-07-12 16:00:44,895] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:44,896] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 31.1 GB, percent = 24.7%
[2024-07-12 16:00:44,976] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2024-07-12 16:00:44,976] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:44,977] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 31.1 GB, percent = 24.7%
[2024-07-12 16:00:45,811] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1
[2024-07-12 16:00:45,811] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:45,812] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 31.91 GB, percent = 25.4%
[2024-07-12 16:00:45,892] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2024-07-12 16:00:45,893] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:45,894] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 31.91 GB, percent = 25.4%
[2024-07-12 16:00:45,990] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2024-07-12 16:00:45,990] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:45,991] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 32.01 GB, percent = 25.5%
[2024-07-12 16:00:46,073] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2024-07-12 16:00:46,074] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:46,075] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 32.98 GB, percent = 26.2%
[2024-07-12 16:00:46,922] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2024-07-12 16:00:46,923] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.9 GB         Max_CA 2 GB 
[2024-07-12 16:00:46,924] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 33.31 GB, percent = 26.5%
[2024-07-12 16:00:46,925] [INFO] [stage3.py:392:_setup_for_real_optimizer] optimizer state initialized
[2024-07-12 16:00:57,244] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2024-07-12 16:00:57,245] [INFO] [utils.py:786:see_memory_usage] MA 0.13 GB         Max_MA 1.35 GB         CA 2.52 GB         Max_CA 3 GB 
[2024-07-12 16:00:57,246] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 33.8 GB, percent = 26.9%
[2024-07-12 16:00:57,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-07-12 16:00:57,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2024-07-12 16:00:57,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x2af9e4109fa0>
[2024-07-12 16:00:57,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-07-12 16:00:57,248] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2024-07-12 16:00:57,248] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-12 16:00:57,248] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-12 16:00:57,248] [INFO] [config.py:959:print]   amp_enabled .................. False
[2024-07-12 16:00:57,248] [INFO] [config.py:959:print]   amp_params ................... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2af9e76fa7c0>
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   communication_data_type ...... None
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   disable_allgather ............ False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   dump_state ................... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... None
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2024-07-12 16:00:57,249] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   fp16_auto_cast ............... None
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   fp16_enabled ................. False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   global_rank .................. 0
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   gradient_clipping ............ 0.0
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   loss_scale ................... 0
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   optimizer_name ............... adamw
[2024-07-12 16:00:57,250] [INFO] [config.py:959:print]   optimizer_params ............. {'lr': 1e-05, 'weight_decay': 1e-05}
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   pld_enabled .................. False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   pld_params ................... False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   scheduler_name ............... None
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   scheduler_params ............. None
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   sparse_attention ............. None
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   steps_per_print .............. 10
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   train_batch_size ............. 1024
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  4
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   world_size ................... 256
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  False
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   zero_enabled ................. True
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-12 16:00:57,251] [INFO] [config.py:959:print]   zero_optimization_stage ...... 3
[2024-07-12 16:00:57,251] [INFO] [config.py:945:print_user_config]   json = {
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "fp16": {
        "enabled": false, 
        "auto_cast": false, 
        "loss_scale": 0, 
        "initial_scale_power": 32, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "weight_decay": 1e-05
        }
    }, 
    "train_batch_size": 1.024000e+03, 
    "train_micro_batch_size_per_gpu": 4, 
    "wall_clock_breakdown": false
}


  0%|          | 0/19345 [00:00<?, ?it/s]I0712 16:01:22.079099 21814 ProcessGroupNCCL.cpp:1274] NCCL_DEBUG: N/A


  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:51, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:51, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:36, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:36, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:32, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:32, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:21, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:21, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:11, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:11, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:57, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:57, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:34, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:34, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:08, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:08, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:40, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:40, 55.48s/it]
  0%|          | 2/19345 [01:25<217:38:57, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:57, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:07, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:07, 35.84s/it]
  0%|          | 4/19345 [02:26<180:36:19, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:19, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:01, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:01, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:55, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:55, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:04, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:04, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:16, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:16, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:11, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:11, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  
  0%|          | 1/19345 [00:55<297:17:14, 55.33s/it]
                                                     

  0%|          | 1/19345 [00:55<297:17:14, 55.33s/it]
  0%|          | 2/19345 [01:25<217:19:20, 40.45s/it]
                                                     

  0%|          | 2/19345 [01:25<217:19:20, 40.45s/it]
  0%|          | 3/19345 [01:55<192:24:24, 35.81s/it]
                                                     

  0%|          | 3/19345 [01:55<192:24:24, 35.81s/it]
  0%|          | 4/19345 [02:25<180:29:32, 33.60s/it]
                                                     

  0%|          | 4/19345 [02:25<180:29:32, 33.60s/it]
  0%|          | 5/19345 [02:56<173:56:58, 32.38s/it]
                                                     

  0%|          | 5/19345 [02:56<173:56:58, 32.38s/it]
  0%|          | 6/19345 [03:26<170:12:17, 31.68s/it]
                                                     

  0%|          | 6/19345 [03:26<170:12:17, 31.68s/it]
  0%|          | 7/19345 [03:56<167:24:11, 31

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:04:58, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:04:58, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:33, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:33, 40.51s/it]
  0%|          | 3/19345 [01:55<192:34:56, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:34:56, 35.84s/it]
  0%|          | 4/19345 [02:25<180:35:57, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:35:57, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:03, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:03, 32.39s/it]
  0%|          | 6/19345 [03:26<170:14:59, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:14:59, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:53, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:53, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:23, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:23, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:18, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:18, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:25, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:25, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:11, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:11, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:24, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:24, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:00, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:00, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:48, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:48, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:42, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:42, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:14, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:14, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:04:28, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:04:28, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:34:55, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:34:55, 35.84s/it]
  0%|          | 4/19345 [02:25<180:35:57, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:35:57, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:01, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:01, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:02, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:02, 55.48s/it]
  0%|          | 2/19345 [01:25<217:38:58, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:58, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:14, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:14, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:01, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:01, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:46, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:46, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:34, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:34, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:19, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:19, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:04:56, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:04:56, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:55, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:55, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:03, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:03, 35.84s/it]
  0%|          | 4/19345 [02:25<180:35:57, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:35:57, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:53, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:53, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:17, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:17, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:41, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:41, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:24, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:24, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:14, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:14, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:14, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:14, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:40, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:40, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:06, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:06, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:04, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:04, 35.84s/it]
  0%|          | 4/19345 [02:26<180:36:06, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:06, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:39, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:39, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:18, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:18, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:15, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:15, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:30, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:30, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:08, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:08, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:41, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:41, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:23, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:23, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:32, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:32, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:01, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:01, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:50, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:50, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:43, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:43, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:09, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:09, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:17, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:17, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:06, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:06, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:55, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:55, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:04, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:04, 35.84s/it]
  0%|          | 4/19345 [02:25<180:36:01, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:01, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:10, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:10, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:30, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:30, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:31, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:31, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:20, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:20, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:09, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:09, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:04:37, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:04:37, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:43, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:43, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:01, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:01, 35.84s/it]
  0%|          | 4/19345 [02:25<180:36:03, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:03, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:03, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:03, 32.39s/it]
  0%|          | 6/19345 [03:26<170:14:59, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:14:59, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:09, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:09, 55.48s/it]
  0%|          | 2/19345 [01:25<217:38:56, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:56, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:06, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:06, 35.84s/it]
  0%|          | 4/19345 [02:26<180:36:05, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:05, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:08, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:08, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:45, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:45, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:19, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:19, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:21, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:21, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:28, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:28, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:08, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:08, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:08, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:08, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:58, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:58, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:16, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:16, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:16, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:16, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:08, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:08, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:02, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:57, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:57, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:33, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:33, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:19, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:19, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:21, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:21, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:27, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:27, 55.48s/it]
  0%|          | 2/19345 [01:25<217:38:48, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:48, 40.51s/it]
  0%|          | 3/19345 [01:55<192:34:58, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:34:58, 35.84s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:01, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:01, 32.39s/it]
  0%|          | 6/19345 [03:26<170:14:57, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:14:57, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:21, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:21, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:41, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:41, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:29, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:29, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:09, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:09, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:43, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:43, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:40, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:40, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:32, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:32, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:12, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:12, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:51, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:51, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:41, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:41, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:19, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:19, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:05:05, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:05:05, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:54, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:54, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:06, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:06, 35.84s/it]
  0%|          | 4/19345 [02:26<180:36:05, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:05, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:02, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:02, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:01, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:01, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:14, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:14, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:10, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:10, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:14, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:14, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:24, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:24, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:20, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:20, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:11, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:11, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:09, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:09, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:12, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:12, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:40, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:40, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:09, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:09, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:54, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:54, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:43, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:43, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:13, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:13, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:24, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:24, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:49, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:49, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:30, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:30, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:21, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:21, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:36, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:36, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:19, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:19, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:27, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:27, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:48, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:48, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:13, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:12, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:12, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:48, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:48, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:29, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:29, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:22, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:22, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:21, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:21, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:33, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:33, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:04:48, 55.47s/it]
                                                     

  0%|          | 1/19345 [00:55<298:04:48, 55.47s/it]
  0%|          | 2/19345 [01:25<217:38:54, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:38:54, 40.51s/it]
  0%|          | 3/19345 [01:55<192:34:56, 35.84s/it]
                                                     

  0%|          | 3/19345 [01:55<192:34:56, 35.84s/it]
  0%|          | 4/19345 [02:25<180:36:02, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:02, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:08, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:08, 32.39s/it]
  0%|          | 6/19345 [03:26<170:14:57, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:14:57, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:09, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:09, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:30, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:30, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:26, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:26, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:10, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:10, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:40, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:40, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:42, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:42, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:43, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:43, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:19, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:19, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:56, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:56, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:48, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:48, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:31, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:49, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:49, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:32, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:32, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:26, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:03, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:03, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:11, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:11, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:15, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:15, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:11, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:11, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:07, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:03, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:45, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:45, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:28, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:18, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:14, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:14, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:38, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:27, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:14, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:14, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:05, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:57, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:57, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:45, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:45, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:23, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:23, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:08, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:08, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:06, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:06, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:46, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:46, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:32, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:32, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:13, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:08, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:08, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:01, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:01, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:27, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:27, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:18, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:18, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:23, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:23, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:10, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:10, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:11, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:11, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:04, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:27, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:27, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:21, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:21, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:08, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:08, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:40, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:40, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:29, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:29, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:20, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:20, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:21, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:21, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:07, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:55, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:55, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:44, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:44, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:29, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:29, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:10, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:41, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:41, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:37, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:24, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:15, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:16, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:12, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:12, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:33, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:33, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:33, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:33, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:10, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:10, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:06, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:47, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:47, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:46, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:46, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:14, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:15, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:07:13, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:07:13, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:39, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:25, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:25, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:24, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:24, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:18, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  

  0%|          | 0/19345 [00:00<?, ?it/s]
  0%|          | 1/19345 [00:55<298:06:46, 55.48s/it]
                                                     

  0%|          | 1/19345 [00:55<298:06:46, 55.48s/it]
  0%|          | 2/19345 [01:25<217:39:40, 40.51s/it]
                                                     

  0%|          | 2/19345 [01:25<217:39:40, 40.51s/it]
  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
                                                     

  0%|          | 3/19345 [01:55<192:35:30, 35.85s/it]
  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
                                                     

  0%|          | 4/19345 [02:26<180:36:16, 33.62s/it]
  0%|          | 5/19345 [02:56<174:01:19, 32.39s/it]
                                                     

  0%|          | 5/19345 [02:56<174:01:19, 32.39s/it]
  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
                                                     

  0%|          | 6/19345 [03:26<170:15:04, 31.69s/it]
  {'loss': 3.2781, 'learning_rate': 1.034126163391934e-08, 'epoch': 0.0}
{'loss': 3.3248, 'learning_rate': 2.068252326783868e-08, 'epoch': 0.0}
{'loss': 3.2635, 'learning_rate': 3.1023784901758017e-08, 'epoch': 0.0}
{'loss': 3.2959, 'learning_rate': 4.136504653567736e-08, 'epoch': 0.0}
{'loss': 3.3592, 'learning_rate': 5.1706308169596694e-08, 'epoch': 0.0}
{'loss': 3.2752, 'learning_rate': 6.204756980351603e-08, 'epoch': 0.0}
{'loss': 3.1981, 'learning_rate': 7.238883143743538e-08, 'epoch': 0.0}
{'loss': 3.1947, 'learning_rate': 8.273009307135472e-08, 'epoch': 0.0}
{'loss': 3.2848, 'learning_rate': 9.307135470527406e-08, 'epoch': 0.0}
[2024-07-12 16:06:24,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0341261633919339e-07], mom=[(0.9, 0.999)]
[2024-07-12 16:06:24,274] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=34.30316247086527, CurrSamplesPerSec=34.311314946461316, MemAllocated=0.13GB, MaxMemAllocated=5.92GB
0%|          | 7/19345 [03:56<167:25:57, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:25:57, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:21, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:21, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:39, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:39, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:56, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:56, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:38, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:38, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:13, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:13, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:17, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:17, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:46, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:46, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:28, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:28, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:57, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:57, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:46, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:46, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:56, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:56, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:20, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:20, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:39, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:39, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:13, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:13, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:56, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:56, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:21, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:21, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:38, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:38, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:01, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:01, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:28, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:28, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:43, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:43, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:07, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:07, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:44, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:44, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:15, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:15, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:07, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:07, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:07, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:07, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:21, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:21, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:57, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:57, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:01, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:01, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:26, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:26, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:02, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:02, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:02, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:02, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:10, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:10, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:57, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:57, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:23, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:23, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:44, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:44, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:25:58, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:25:58, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:19, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:07, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:07, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:17, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:17, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:39, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:39, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:59, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:59, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:25:59, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:19, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:19, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:43, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:43, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:02, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:02, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:20, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:20, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:46, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:46, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:23, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:23, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:57, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:57, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:04, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:04, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:12, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:12, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:02, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:02, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:04, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:04, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:03, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:23, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:23, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:58, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:02, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:00, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:00, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:37, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:37, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:27, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:27, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:02, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:02, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:55, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:55, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:01, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:04, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:04, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:27, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:43, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:43, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:00, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:24, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:40, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:59, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:59, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:05, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:05, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:41, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:00, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:00, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:22, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:15, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:15, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:59, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:05, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:05, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:25, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:44, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:44, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:18, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:08, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:30, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:30, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:14, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:01, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:01, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:59, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:59, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:04, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:04, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:28, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:28, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:38, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:38, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:15, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:15, 30.45s/it]
  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:08:00, 30.38s/it]
  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
                                                      

  0%|          | 12/19345 [06:27<162:48:03, 30.32s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:11, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:11, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:26, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:26, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:51, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:51, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
                                                      

  0%|          | 10/19345 [05:26<163:31:16, 30.45s/it]
  0%|          | 11/19345 [05:57<163:07:55, 30.38s/it]
                                                      

  0%|          | 11/19345 [05:57<163:07:55, 30.38s/it]
  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
                                                      

  0%|          | 12/19345 [06:27<162:47:58, 30.31s/it]
  0%|          | 13/19345 [06:57<162:310%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
                                                     

  0%|          | 7/19345 [03:56<167:26:06, 31.17s/it]
  0%|          | 8/19345 [04:26<165:39:29, 30.84s/it]
                                                     

  0%|          | 8/19345 [04:26<165:39:29, 30.84s/it]
  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
                                                     

  0%|          | 9/19345 [04:56<164:13:42, 30.58s/it]
  0%|          | 10/19345 [05:26<163:31:17, 30.45s/it]
                                                      
