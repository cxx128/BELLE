# 项目说明

本仓库用于微调Bloom和Llama两个大语言模型，并且支持两种训练模式：Deepspeed和 LoRA

## 环境安装

```bash
conda env create -f environment.yml
conda activate Belle
conda install -c nvidia libcusolver-dev
```

## 数据下载

```bash
python download_data.py
```

创建data_dir文件夹，并且下载我们参考[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) 生成的中文数据集[1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)，同时随机地划分训练和测试数据

## 训练模型

训练模型的配置文件存放在run_config文件夹中

- Bloom_config.json: 配置Bloom模型的超参数
- Llama_config.json: 配置Llama模型的超参数
- deepspeed_config.json: 配置Deepspeed策略的参数
- lora_hyperparams_bloom.json: LoRA策略训练Bloom模型的参数
- lora_hyperparams_llama.json: LoRA策略训练Llama模型的参数

### Deepspeed

使用deepspeed命令运行训练脚本

训练Bloom模型的启动命令：

```bash
deepspeed --num_gpus=8 finetune.py --model_config_file run_config/Bloom_config.json  --deepspeed run_config/deepspeed_config.json 
```

训练Llama模型的启动命令：

```bash
deepspeed --num_gpus=8 finetune.py --model_config_file run_config/Llama_config.json  --deepspeed run_config/deepspeed_config.json 
```

### LoRA

如果采用LoRA，需要使用torchrun命令启动分布式训练(使用deepspeed启动会出现错误)，同时需要指定use_lora参数并且给出LoRA需要的参数配置文件lora_hyperparams_file

采用LoRA训练Bloom模型的启动命令:

```bash
torchrun --nproc_per_node=8 finetune.py --model_config_file run_config/Bloom_config.json --lora_hyperparams_file run_config/lora_hyperparams_bloom.json  --use_lora
```

采用LoRA训练Llama模型的启动命令:

```bash
torchrun --nproc_per_node=8 finetune.py --model_config_file run_config/Llama_config.json --lora_hyperparams_file run_config/lora_hyperparams_llama.json  --use_lora
```

## 文本生成

训练的模型将会保存在trained_models/model_name目录下，其中model_name是模型名，比如Bloom，Llama。假设训练的模型是Bloom，训练数据采用的是Belle_open_source_0.5M，下面的命令将读取模型并生成测试集中每一个样本的生成结果

```bash
python generate.py --dev_file data_dir/Belle_open_source_0.5M.dev.json --model_name_or_path trained_models/bloom/
```

如果是LoRA模型，需要给出LoRA权重保存的位置，如：--lora_weights trained_models/lora-llama

## 参考

本仓库的代码基于[alpaca-lora](https://github.com/tloen/alpaca-lora)



# Usage

This repository is used to fine-tune the Bloom and Llama large language models, and supports two training modes: Deepspeed and LoRA.

## Environment Setup

```bash
conda env create -f environment.yml
conda activate Belle
conda install -c nvidia libcusolver-dev
```

## Data Download

```bash
python download_data.py
```

Create the `data_dir` folder and download the Chinese dataset [1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) generated by [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca). The training and testing data are randomly split.

## Model Training

The model training configuration files are stored in the `run_config` folder.

- `Bloom_config.json`: hyperparameters for the Bloom model
- `Llama_config.json`: hyperparameters for the Llama model
- `deepspeed_config.json`: parameters for the Deepspeed strategy
- `lora_hyperparams_bloom.json`: parameters for training the Bloom model with LoRA strategy
- `lora_hyperparams_llama.json`: parameters for training the Llama model with LoRA strategy

### Deepspeed 

Use the `deepspeed` command to run the training script.

Command to train the Bloom model:

```bash
deepspeed --num_gpus=8 finetune.py --model_config_file run_config/Bloom_config.json  --deepspeed run_config/deepspeed_config.json 
```

Command to train the Llama model:

```bash
deepspeed --num_gpus=8 finetune.py --model_config_file run_config/Llama_config.json  --deepspeed run_config/deepspeed_config.json 
```


### Lora

If using LoRA, start the distributed training using the `torchrun` command (an error will occur if starting with `deepspeed`). Also, the `use_lora` parameter needs to be specified, and the `lora_hyperparams_file` file that LoRA needs should be provided.

Command to train the Bloom model using LoRA:

```bash
torchrun --nproc_per_node=8 finetune.py --model_config_file run_config/Bloom_config.json --lora_hyperparams_file run_config/lora_hyperparams_bloom.json  --use_lora
```

Command to train the Llama model using LoRA:

```bash
torchrun --nproc_per_node=8 finetune.py --model_config_file run_config/Llama_config.json --lora_hyperparams_file run_config/lora_hyperparams_llama.json  --use_lora
```

## Text Generation

The trained models will be saved in the `trained_models/model_name` directory, where `model_name` is the name of the model, such as Bloom or Llama. Assuming that the trained model is Bloom and the training data used is Belle_open_source_0.5M, the following command will read the model and generate the results for each sample in the test set.

```bash
python generate.py --dev_file data_dir/Belle_open_source_0.5M.dev.json --model_name_or_path trained_models/bloom/
```

If it is a LoRA model, the location where the LoRA weights are saved needs to be provided, such as `--lora_weights trained_models/lora-llama`.

## Reference

The code in this repository is based on [alpaca-lora](https://github.com/tloen/alpaca-lora).
