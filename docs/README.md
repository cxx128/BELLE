# This folder contains recent research papers from BELLE GROUP.


[1] Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, Xiangang Li, "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences", arXiv preprint [arXiv:2303.07610](https://github.com/LianjiaTech/BELLE/blob/main/docs/Exploring%20ChatGPT's%20Ability%20to%20Rank%20Content%20A%20Preliminary%20Study%20on%20Consistency%20with%20Human%20Preferences.pdf).

* **Abstract**
As a natural language assistant, ChatGPT is capable of performing various tasks, including but not limited to arti- cle generation, code completion, and data analysis. Further- more, ChatGPT has consistently demonstrated a remarkable level of accuracy and reliability in terms of content evalu- ation, exhibiting the capability of mimicking human pref- erences. To further explore ChatGPT’s potential in this re- gard, a study is conducted to assess its ability to rank con- tent. In order to do so, a test set consisting of prompts is created, covering a wide range of use cases, and five models are utilized to generate corresponding responses. ChatGPT is then instructed to rank the responses generated by these models. The results on the test set show that ChatGPT’s ranking preferences are consistent with human to a certain extent. This preliminary experimental finding implies that ChatGPT’s zero-shot ranking capability could be used to re-duce annotation pressure in a number of ranking tasks.

<br/>
[2] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, Xiangang Li, "Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases", arXiv preprint [arXiv:2303.14742](https://github.com/LianjiaTech/BELLE/blob/main/docs/Exploring%20the%20Impact%20of%20Instruction%20Data%20Scaling%20on%20Large%20Language%20Models%20An%20Empirical%20Study%20on%20Real-World%20Use%20Cases.pdf) .

* **Abstract**
The success of ChatGPT has recently attracted numer- ous efforts to replicate it, with instruction-tuning strate- gies being a key factor in achieving remarkable re- sults. Instruction-tuning not only significantly enhances the model’s performance and generalization but also makes the model’s generated results more consistent with human speech patterns. However current research rarely studies the impact of different amounts of instruction data on model performance, especially in the real-world use cases. In this paper we explore the performance of large language mod- els based on instruction tuning across different scales of in- struction data. An evaluation dataset consisting of 12 ma- jor online use cases is constructed in the experiment. With Bloomz-7B1-mt as the base model, the results show that 1) merely increasing the amount of instruction data leads to continuous improvement in tasks such as open-ended gener- ation, 2) in tasks such as math and code, the model perfor- mance curve remains quite flat while increasing data size. We further analyze the possible causes of these phenomena and propose potential future research directions such as ef- fectively selecting high-quality training data, scaling base models and training methods specialized for hard tasks.

<br/>
[3] Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang Ma, Xiangang Li, "Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation", [arXiv](https://github.com/LianjiaTech/BELLE/blob/main/docs/Towards%20Better%20Instruction%20Following%20Language%20Models%20for%20Chinese.pdf)

* **Abstract**: 
Recently, significant public efforts have been directed towards developing low-cost models with capabilities akin to ChatGPT, thereby fostering the growth of open- source conversational models. However, there remains a scarcity of comprehensive and in-depth evaluations of these mod- els’ performance. In this study, we exam- ine the influence of training data factors, including quantity, quality, and linguistic distribution, on model performance. Our analysis is grounded in several publicly ac- cessible, high-quality instruction datasets, as well as our own Chinese multi-turn conversations. We assess various mod- els using a evaluation set of 1,000 sam- ples, encompassing nine real-world sce- narios. Our goal is to supplement manual evaluations with quantitative analyses, of- fering valuable insights for the continued advancement of open-source chat mod- els. Furthermore, to enhance the perfor- mance and training/inference efficiency of models in the Chinese domain, we extend the vocabulary of LLaMA – the model with the closest open-source performance to proprietary language models like GPT- 3 – and conduct secondary pre-training on 3.4B Chinese words.
<br/>

[4] Xianghui Sun, Yunjie Ji, Baochang Ma*, Xiangang Li, "A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model", [arXiv](https://github.com/LianjiaTech/BELLE/blob/main/docs/A%20Comparative%20Study%20between%20Full-Parameter%20and%20LoRA-based.pdf)

* **Abstract**
Recently, the instruction-tuning of large language models is a crucial area of re- search in the field of natural language processing. Due to resource and cost limitations, several researchers have em- ployed parameter-efficient tuning tech- niques, such as LoRA, for instruction tun- ing, and have obtained encouraging re- sults In comparison to full-parameter fine- tuning, LoRA-based tuning demonstrates salient benefits in terms of training costs. In this study, we undertook experimental comparisons between full-parameter fine- tuning and LoRA-based tuning methods, utilizing LLaMA as the base model.
The experimental results show that the se- lection of the foundational model, training dataset scale, learnable parameter quan- tity, and model training cost are all im- portant factors. We hope that the experi- mental conclusions of this paper can pro- vide inspiration for training large language models, especially in the field of Chinese, and help researchers find a better trade-off strategy between training cost and model performance